%SIPLIB citation
@article{SIPLIB,
author={S. Ahmed and R. Garcia and N. Kong and L. Ntaimo and G. Parija and F. Qiu and S. Sen},
title={{SIPLIB}: A Stochastic Integer Programming Test Problem Library},
url={http://www.isye.gatech.edu/~sahmed/siplib}, 
year={2015},
}


%abstract="We propose a bundle trust-region algorithm to minimize locally Lipschitz functions which are potentially nonsmooth and nonconvex. We prove global convergence of our method and show by way of an example that the classical convergence argument in trust-region methods based on the Cauchy point fails in the nonsmooth setting. Our method is tested experimentally on three problems in automatic control.",
@Article{Apkarian2016,
author="Apkarian, P. and Noll, D. and Ravanbod, L.",
title="Nonsmooth Bundle Trust-region Algorithm with Applications to Robust Stability",
journal="Set-Valued and Variational Analysis",
year="2016",
volume="24",
number="1",
pages="115--148",
}
%author="Apkarian, Pierre and Noll, Dominikus and Ravanbod, Laleh",
%issn="1877-0541",
%doi="10.1007/s11228-015-0352-5",
%url="http://dx.doi.org/10.1007/s11228-015-0352-5"


%abstract="This paper presents two methods for the optimization of structured large-scale problems: a decomposition method of dual type for nonlinear problems and a sequential quadratic programming based method. Practical details of application of the methods to the case study problem of the hydropower system of an African river are then given. Comparison of results is presented, indicating that both methods are useful and efficient, having however different features from a practical point of view. General remarks concerning the practical differences between a decomposition-based method and a method exploiting the problem structure within the framework of general purpose optimization routines are finally presented.",
@Article{ArnoldEtAl1994,
author={Arnold, E. and Tatjewski, P. and Wo{\l}ochowicz, P.},
title={Two methods for large-scale nonlinear optimization and their comparison on a case study of hydropower optimization},
journal={Journal of Optimization Theory and Applications},
volume={81},
number={2},
pages={221--248},
year={1994},
}
%issn="1573-2878",
%doi="10.1007/BF02191662",
%url="http://dx.doi.org/10.1007/BF02191662"



%We present a convex nondifferentiable minimization algorithm of proximal bundle type that does not rely on measuring descent of the objective function to declare the so-called serious steps; rather, a merit function is defined which is decreased at each iteration, leading to a (potentially) continuous choice of the stepsize between zero (the null step) and one (the serious step). By avoiding the discrete choice the convergence analysis is simplified, and we can more easily obtain efficiency estimates for the method. Some choices for the step selection actually reproduce the dichotomic behavior of standard proximal bundle methods but shed new light on the rationale behind the process, and ultimately with different rules; furthermore, using nonlinear upper models of the function in the step selection process can lead to actual fractional steps.
@article{AstorinoEtAl2013,
author = {A. Astorino and A. Frangioni and A. Fuduli and E. Gorgone},
title = {A Nonmonotone Proximal Bundle Method with (Potentially) Continuous Step Decisions},
journal = {SIAM Journal on Optimization},
volume = {23},
number = {3},
pages = {1784-1809},
year = {2013},
}
%doi = {10.1137/120888867},
%
%URL = { 
%        http://dx.doi.org/10.1137/120888867
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/120888867
%    
%}



%abstract="One of the main drawbacks of the augmented Lagrangian relaxation method is that the quadratic term introduced by the augmented Lagrangian is not separable. We compare empirically and theoretically two methods designed to cope with the nonseparability of the Lagrangian function: the auxiliary problem principle method and the block coordinated descent method. Also, we use the so-called unit commitment problem to test both methods. The objective of the unit commitment problem is to optimize the electricity production and distribution, considering a short-term planning horizon.",
@Article{BeltranHeredia2002,
author={Beltran, C. and Heredia, F. J.},
title={Unit Commitment by Augmented {L}agrangian Relaxation: Testing Two Decomposition Approaches},
journal={Journal of Optimization Theory and Applications},
volume={112},
number={2},
pages={295--314},
year={2002},
}
%issn="1573-2878",
%doi="10.1023/A:1013601906224",
%url="http://dx.doi.org/10.1023/A:1013601906224"

@article{BenAmor2009SCG,
title = {On the choice of explicit stabilizing terms in column generation},
journal = {Discrete Applied Mathematics},
volume = {157},
number = {6},
pages = {1167 -- 1184},
year = {2009},
author = {H.M.T. Ben Amor and J. Desrosiers and A. Frangioni},
}

@article{BergerEtAl1994,
author = {A.J. Berger and J.M. Mulvey and A. Ruszczy\'{n}ski},
title = {An Extension of the {DQA} Algorithm to Convex Stochastic Programs},
journal = {SIAM Journal on Optimization},
volume = {4},
number = {4},
pages = {735-753},
year = {1994},
}
%author = {Adam J. Berger and John M. Mulvey and Andrzej Ruszczyński},
%doi = {10.1137/0804043},
%
%URL = { 
%        http://dx.doi.org/10.1137/0804043
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/0804043
%    
%}
%The diagonal quadratic approximation (DQA) algorithm is extended for the case of risk-averse utility and other nonlinear functions associated with stochastic programs. The method breaks the stochastic program into a sequence of smaller quadratic programming subproblems that can be executed in parallel. Each subproblem is solved approximately by means of a convex version of a primal-dual interior-point code (LOQO). Convergence of the distributed DQA method is discussed.
%All communication takes place among neighboring processors rather than via a master routine leading to an efficient distributed implementation. Results with a realworld airline planning model possessing a convex objective, 155,320 linear constraints and 303,600 variables, show the DQA algorithm’s efficiency. The interior point direct solver (convex-LOQO) is shown to solve moderate-size stochastic programs in a small number of iterations (under 50).


@book{Bertsekas1982,
   AUTHOR = {Bertsekas, D.P.},
    TITLE = {Constrained Optimization and Lagrange Multiplier Methods},
PUBLISHER = {Academic Press},
     YEAR = {1982},
}

@book{BerTsi1989,
author = {D. P. Bertsekas and J. N. Tsitsiklis},
title = {Parallel and Distributed Computation, Numerical Methods},
year = {1989},
publisher = {Prentice-Hall, Inc., Upper Saddle River, NJ},
} 

@book{Bertsekas1999,
   AUTHOR = {Bertsekas, D.P.},
    TITLE = {Nonlinear Programming},
PUBLISHER = {Athena Scientific},
     YEAR = {1999},
}



%abstract="We consider the minimization of a sum   \$\$\{{\backslash}sum\_\{i=1\}^mf\_i(x)\}\$\$  consisting of a large number of convex component functions f                                          i                . For this problem, incremental methods consisting of gradient or subgradient iterations applied to single components have proved very effective. We propose new incremental methods, consisting of proximal iterations applied to single components, as well as combinations of gradient, subgradient, and proximal iterations. We provide a convergence and rate of convergence analysis of a variety of such methods, including some that involve randomization in the selection of components. We also discuss applications in a few contexts, including signal processing and inference/machine learning.",
@article{Bertsekas2011,
author={Bertsekas, Dimitri P.},
title={Incremental proximal methods for large scale convex optimization},
journal={Mathematical Programming},
year={2011},
volume={129},
number={2},
pages={163--195},
}
%issn="1436-4646",
%doi="10.1007/s10107-011-0472-0",
%url="http://dx.doi.org/10.1007/s10107-011-0472-0"



%We consider minimization of the sum of a large number of convex functions, and we propose an incremental aggregated version of the proximal algorithm, which bears similarity to the incremental aggregated gradient and subgradient methods that have received a lot of recent attention. Under cost function differentiability and strong convexity assumptions, we show linear convergence for a sufficiently small constant stepsize. This result also applies to distributed asynchronous variants of the method, involving bounded interprocessor communication delays. 
%We then consider dual versions of incremental proximal algorithms, which are incremental augmented Lagrangian methods for separable equality-constrained optimization problems. Contrary to the standard augmented Lagrangian method, these methods admit decomposition in the minimization of the augmented Lagrangian, and update the multipliers far more frequently. Our incremental aggregated augmented Lagrangian methods bear similarity to several known decomposition algorithms, including the alternating direction method of multipliers (ADMM) and more recent variations. We compare these methods in terms of their properties, and highlight their potential advantages and limitations. 
%We also address the solution of separable inequality-constrained optimization problems through the use of nonquadratic augmented Lagrangiias such as the exponential, and we dually consider a corresponding incremental aggregated version of the proximal algorithm that uses nonquadratic regularization, such as an entropy function. We finally propose a closely related linearly convergent method for minimization of large differentiable sums subject to an orthant constraint, which may be viewed as an incremental aggregated version of the mirror descent method.
@misc{Bertsekas2015,
author={Bertsekas, D.P.},
title={Incremental aggregated proximal and augmented {L}agrangian algorithms},
howpublished={arXiv preprint arXiv:1509.09257},
year={2015},
}

@book{Bertsekas2015Text,
author={Bertsekas, D.P.},
title={Convex Optimization Algorithms},
publisher={Athena Scientific},
year={2015},
}



@article{BezdekAtAl1987,
year={1987},
journal={Journal of Optimization Theory and Applications},
volume={54},
issue={3},
title={Local convergence analysis of a grouped variable version of coordinate descent},
publisher={Kluwer Academic Publishers-Plenum Publishers},
author={Bezdek, J.C. and Hathaway, R.J. and Howard, R.E. and Wilson, C.A. and Windham, M.P.},
pages={471-477},
}

@article{BezdekHathaway2003,
 author = {Bezdek, J.C. and Hathaway, R.J.},
 title = {Convergence of Alternating Optimization},
 journal = {Neural, Parallel Sci. Comput.},
 issue_date = {December 2003},
 volume = {11},
 number = {4},
 year = {2003},
 pages = {351--368},
} 
% author = {Bezdek, James C. and Hathaway, Richard J.},
%  month = dec,
%  issn = {1061-5369},
% numpages = {18},
% url = {http://dl.acm.org/citation.cfm?id=964885.964886},
% acmid = {964886},
% publisher = {Dynamic Publishers, Inc.},
% address = {Atlanta, GA, USA},
% keywords = {alternating optimization, block nonlinear Gauss-Seidel, block relaxation, grouped coordinate descent},


@book{BirgeLouveaux2011,
  title={Introduction to Stochastic Programming},
  author={Birge, John R and Louveaux, Francois},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{Bodur2014EtAl,
  title={Strengthened {B}enders Cuts for Stochastic Integer Programs with Continuous Recourse},
  author={Bodur, M. and Dash, S. and G{\"u}nl{\"u}k, O. and Luedtke, J.},
  url={http://www.optimization-online.org/DB\_FILE/2014/03/4263.pdf},
  note={Last accessed on 13 January 2015},
  year={2014},
}

@article{BolandEtAl2016,
author={N.L. Boland and J. Christiansen and B. Dandurand and A. Eberhard and J. Linderoth and J. Luedtke and F. Oliveira},
title={Progressive hedging with a {F}rank-{W}olfe based method for computing stochastic mixed-integer programming {L}agrangian dual bounds},
journal={Optimization Online},
url={http://www.optimization-online.org/DB\_HTML/2016/03/5391.html},
year={2016},
}


@article{Bonettini2011,
author = {Bonettini, S.}, 
title = {Inexact block coordinate descent methods with application to non-negative matrix factorization},
volume = {31}, 
number = {4}, 
pages = {1431-1452}, 
year = {2011}, 
journal = {IMA Journal of Numerical Analysis},
}
%author = {Bonettini, Silvia}, 


@book{BonnansEtAl2006,
author={Bonnans, J. F. and Gilbert, J. C. and Lemarechal, C. and Sagastizabal, S.C.}, 
year={2006},
title={Numerical Optimization: Theoretical and Practical Aspects}, 
publisher={Springer, NY},
} 




@article{BoydEtAl2011,
 author = {S. Boyd and N. Parikh and E. Chu and B. Peleato and J. Eckstein},
 title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
 journal = {Foundation and Trends in Machine Learning},
 issue_date = {January 2011},
 volume = {3},
 number = {1},
 year = {2011},
 pages = {1--122},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 


@article{caroe1999dual,
	title={Dual decomposition in stochastic integer programming},
	author={Car{\o}e, Claus C and Schultz, R{\"u}diger},
	journal={Operations Research Letters},
	volume={24},
	number={1},
	pages={37--45},
	year={1999},
	publisher={Elsevier},
}


@article{CassioliEtAl2013,
title = {On the convergence of inexact block coordinate descent methods for constrained optimization},
journal = {European Journal of Operational Research},
volume = {231},
number = {2},
pages = {274--281},
year = {2013},
author = {A. Cassioli and D. Di Lorenzo and M. Sciandrone},
}
%note = "",
%issn = "0377-2217",
%doi = "http://dx.doi.org/10.1016/j.ejor.2013.05.049",
%url = "http://www.sciencedirect.com/science/article/pii/S0377221713004669",
%keywords = "Nonlinear programming",
%keywords = "Block coordinate descent methods",
%keywords = "Inexact decomposition methods",
%keywords = "Gradient projection",
%keywords = "Frank–Wolfe "

@article{ChatzipanagiotisEtAl2014,
author={Chatzipanagiotis, N. and Dentcheva, D. and Zavlanos, M.M.},
title={An augmented {L}agrangian method for distributed optimization},
journal={Mathematical Programming},
year={2014},
volume={152},
number={1},
pages={405--434},
}
%author="Chatzipanagiotis, Nikolaos and Dentcheva, Darinka and Zavlanos, Michael M.",
%abstract="We propose a novel distributed method for convex optimization problems with a certain separability structure. The method is based on the augmented Lagrangian framework. We analyze its convergence and provide an application to two network models, as well as to a two-stage stochastic optimization problem. The proposed method compares favorably to two augmented Lagrangian decomposition methods known in the literature, as well as to decomposition methods based on the ordinary Lagrangian function.",
%issn="1436-4646",
%doi="10.1007/s10107-014-0808-7",
%url="http://dx.doi.org/10.1007/s10107-014-0808-7"

@article{ChenTeboulle1994,
author={Chen, G. and Teboulle, M.},
title={A proximal-based decomposition method for convex minimization problems},
journal={Mathematical Programming},
volume={64},
year={1994},
pages={81--101},
}


@book{Clarke1990,
author = {Clarke, F.},
title = {Optimization and Nonsmooth Analysis},
publisher = {Society for Industrial and Applied Mathematics},
year = {1990},
}
%doi = {10.1137/1.9781611971309},
%address = {},
%edition   = {},
%URL = {http://epubs.siam.org/doi/abs/10.1137/1.9781611971309},
%eprint = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611971309}


%abstract="We study the convergence of a diagonal process for minimizing a closed proper convex function f, in which a proximal point iteration is applied to a sequence of functions approximating f. We prove that, when the approximation is sufficiently fast, and also when it is sufficiently slow, the sequence generated by the method converges toward a minimizer of f. Comparison to previous work is provided through examples in penalty methods for linear programming and Tikhonov regularization.",
@Article{Cominetti1997,
author="Cominetti, R.",
title="Coupling the Proximal Point Algorithm with Approximation Methods",
journal="Journal of Optimization Theory and Applications",
year="1997",
volume="95",
number="3",
pages="581--600",
}
%issn="1573-2878",
%doi="10.1023/A:1022621905645",
%url="http://dx.doi.org/10.1023/A:1022621905645"

@article{COIN-OR,
author={R. Lougee-Heimer},
title={{The Common Optimization INterface for Operations Research}}, 
journal={IBM Journal of Research and Development},
volume={47},
number={1},
pages={57-66}, 
year={January 2003},
}
%author={Robin Lougee-Heimer},
@misc{COIN-ORURL,
title={{COmputational INfrastructure for
Operations Research}}, 
url={http://www.coin-or.org/},
note={Last accessed 28 January, 2016},

}

@manual{CPLEX12-5,
title={IBM ILOG CPLEX V12.5},
organization={IBM Corporation},
url={http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/},
note={Last accessed 28 Jan 2016},
}

@manual{CPLEX12-6,
title={IBM ILOG CPLEX Optimization Studio CPLEX User’s Manual},
organization={IBM Corporation},
url={http://www.ibm.com/support/knowledgecenter/en/SSSA5P\_12.6.1/\\ilog.odms.studio.help/pdf/usrcplex.pdf},
note={Last accessed 22 August 2016},
}

@manual{MATLAB2012B,
title={MATLAB 2012b}, 
organization={The MathWorks, Natick},
year={2014},
}


%abstract="We present a simple and unified technique to establish convergence of various minimization methods. These contain the (conceptual) proximal point method, as well as implementable forms such as bundle algorithms, including the classical subgradient relaxation algorithm with divergent series.",
@Article{CorreaLemarechal1993,
author="Correa, R. and Lemar{\'e}chal, C.",
title="Convergence of some algorithms for convex minimization",
journal="Mathematical Programming",
year="1993",
volume="62",
number="1",
pages="261--275",
}
%author="Correa, Rafael and Lemar{\'e}chal, Claude",
%issn="1436-4646",
%doi="10.1007/BF01585170",
%url="http://dx.doi.org/10.1007/BF01585170"


@article{DandurandWiecek2015,
author={B. Dandurand and M.M. Wiecek},
title={Distributed Computation of {P}areto Sets},
journal={SIAM Journal on Optimization},
volume={25},
number={2},
pages={1083--1109},
year={2015},
}


@ARTICLE{DeLeoneEtAl1996,
    author = {R. de Leone and R.R. Meyer and S. Kontogiorgis},
    title = {Alternating Direction Splittings For Block-Angular Parallel Optimization},
    journal = {J. Optim. Theory Appl},
    year = {1996},
    volume = {90},
    pages = {2--9},
}
%author = {Renato De Leone and Robert R. Meyer and Spyridon Kontogiorgis},

@article{EcksteinBertsekas1992,
journal={Mathematical Programming},
volume={55},
number={1-3},
year={1992},
title={On the {D}ouglas-{R}achford splitting method and the proximal point algorithm for maximal monotone operators},
publisher={Springer-Verlag},
author={Eckstein, J. and Bertsekas, D.P.},
pages={293-318},
}

@Article{Eckstein2003,
author="Eckstein, J.",
title="A practical general approximation criterion for methods of multipliers based on {B}regman distances",
journal="Mathematical Programming",
volume="96",
number="1",
pages="61--86",
}
%author="Eckstein, Jonathan",
%abstract="\enspaceThis paper demonstrates that for generalized methods of multipliers for convex programming based on Bregman distance kernels -- including the classical quadratic method of multipliers -- the minimization of the augmented Lagrangian can be truncated using a simple, generally implementable stopping criterion based only on the norms of the primal iterate and the gradient (or a subgradient) of the augmented Lagrangian at that iterate. Previous results in this and related areas have required conditions that are much harder to verify, such as $\epsilon$-optimality with respect to the augmented Lagrangian, or strong conditions on the convex program to be solved. Here, only existence of a KKT pair is required, and the convergence properties of the exact form of the method are preserved. The key new element in the analysis is the use of a full conjugate duality framework, as opposed to mainly examining the action of the method on the standard dual function of the convex program. An existence result for the iterates, stronger than those possible for the exact form of the algorithm, is also included.",
%issn="1436-4646",
%doi="10.1007/s10107-003-0374-x",
%url="http://dx.doi.org/10.1007/s10107-003-0374-x"

@article{EcksteinSilva2013,
author={Eckstein, J. and Silva, P.J.S.},
title={A practical relative error criterion for augmented Lagrangians},
journal={Mathematical Programming},
year={2013},
volume={141},
number={1},
pages={319--348},
}
% author={Eckstein, Jonathan and Silva, Paulo J. S.},
%abstract="This paper develops a new error criterion for the approximate minimization of augmented Lagrangian subproblems. This criterion is practical since it is readily testable given only a gradient (or subgradient) of the augmented Lagrangian. It is also ``relative'' in the sense of relative error criteria for proximal point algorithms: in particular, it uses a single relative tolerance parameter, rather than a summable parameter sequence. Our analysis first describes an abstract version of the criterion within Rockafellar's general parametric convex duality framework, and proves a global convergence result for the resulting algorithm. Specializing this algorithm to a standard formulation of convex programming produces a version of the classical augmented Lagrangian method with a novel inexact solution condition for the subproblems. Finally, we present computational results drawn from the CUTE test set---including many nonconvex problems---indicating that the approach works well in practice.",
%issn="1436-4646",
%doi="10.1007/s10107-012-0528-9",
%url="http://dx.doi.org/10.1007/s10107-012-0528-9"

@techreport{EcksteinYao2015,
author={Eckstein, J. and Yao, W.},
title={Understanding the convergence of the alternating direction method of multipliers: Theoretical and computational perspectives},
institution={Rutgers University},
year={2014},
publisher={{RUTCOR}},
}

%A new approach is presented for decomposition of additive separable, nonconvex optimization problems. The overall problem is splitted into independent subproblems of small dimension, and by coordination of the subproblem minimizers an optimal solution of the overall problem is obtained. The method is based on an augmented Lagrangian to convexify the subproblems and on parameter fixing to make the Lagrangian separable
%The coordination is achieved by a suitable choice of the fixed primal parameters and of the Lagrange multipliers with respect to the coupling constraints. In difference to previous methods each subproblem is influenced by its own multiplier which differs from the usual multiplier by a certain multiplier shift. For different algorithms local convergence within a neighbourhood of a strict local minimizer can be shown. In comparison with known methods the new approach has accelerated convergence properties and more transparent rules for the choice of the penalty parameters and the stepsize parameters at the upper level
@article{Engelmann1992,
author={B. Engelmann},
title={Convexification and decomposition of separable nonconvex optimization problems},
journal={Optimization},
volume={26},
issue={1-2},
year={1992},
pages={61--82},
}


%abstract = "Abstract A method for formulating and solving a decentralized unit commitment problem is presented in this work. The method, which extends the alternating direction method of multipliers (ADMM), is presented along with several heuristics and refinements to mitigate oscillations and traps in local optimality that result from the nonconvexity of unit commitment. We present and discuss the promising results from testing the method on large-scale systems of more than 3000 buses. The scalability observed so far suggests that this method is a practical option for use with large systems and may provide a significant benefit for computational speed. "
@article{FeizollahiEtAl2015,
title = "Large-scale decentralized unit commitment ",
journal = "International Journal of Electrical Power \& Energy Systems ",
volume = "73",
number = "",
pages = "97--106",
year = "2015",
author = "Mohammad Javad Feizollahi and Mitch Costley and Shabbir Ahmed and Santiago Grijalva",
}
%note = "",
%issn = "0142-0615",
%doi = "http://dx.doi.org/10.1016/j.ijepes.2015.04.009",
%url = "http://www.sciencedirect.com/science/article/pii/S014206151500191X",
%keywords = "Unit commitment",
%keywords = "Decentralized optimization",
%keywords = "ADMM ",


%We exhibit useful properties of proximal bundle methods for finding $\min_Sf$, where f and S are convex. We show that they asymptotically find objective subgradients and constraint multipliers involved in optimality conditions, multipliers of objective pieces for max-type functions, and primal and dual solutions in Lagrangian decomposition of convex programs. When applied to Lagrangian relaxation of nonconvex programs, they find solutions to relaxed convexified versions of such programs. Numerical results are presented for unit commitment in power production scheduling.
@article{FeltenmarkKiwiel2000,
author = {S. Feltenmark and K.C. Kiwiel},
title = {Dual Applications of Proximal Bundle Methods, Including Lagrangian Relaxation of Nonconvex Problems},
journal = {SIAM Journal on Optimization},
volume = {10},
number = {3},
pages = {697-721},
year = {2000},
}
%author = {Stefan Feltenmark and Krzysztof C. Kiwiel},
%doi = {10.1137/S1052623498332336},
%
%URL = { 
%        http://dx.doi.org/10.1137/S1052623498332336
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/S1052623498332336
%    
%}




%Abstract. An algorithmic framework is presented for optimizing general convex functions by
%nonsynchronized parallel processes. Each process greedily picks a suitable adaptive subset of coordinates
%and runs a bundle method on a corresponding restricted problem stopping whenever a descent
%step is encountered or predicted decrease is reduced sufficiently. No prior knowledge on the dependencies
%between variables is assumed. Instead, dependency information is collected automatically
%by analyzing aggregate subgradient properties required for ensuring convergence. Within this framework
%three strategies are discussed for supporting varying scenarios of structural independence: a
%single convex function, the sum of partially separable convex functions, and a scenario tuned to problem
%decomposition by Lagrangian relaxation of packing-type constraints. The theoretical framework
%presented here generalizes a practical method proposed by the authors for Lagrangian relaxation of
%large scale packing problems and simplifies the analysis.
@article{FischerHelmberg2014,
author = {F. Fischer and C. Helmberg},
title = {A Parallel Bundle Framework for Asynchronous Subspace Optimization of Nonsmooth Convex Functions},
journal = {SIAM Journal on Optimization},
volume = {24},
number = {2},
pages = {795--822},
year = {2014},
}
%author = {Frank Fischer and Christoph Helmberg}
%doi = {10.1137/120865987},
%
%URL = { 
%        http://dx.doi.org/10.1137/120865987
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/120865987
%    
%}




%abstract="In this paper we investigate the effects of replacing the objective function of a 0-1 mixed-integer convex program (MIP) with a ``proximity'' one, with the aim of using a black-box solver as a refinement heuristic. Our starting observation is that enumerative MIP methods naturally tend to explore a neighborhood around the solution of a relaxation. A better heuristic performance can however be expected by searching a neighborhood of an integer solution---a result that we obtain by just modifying the objective function of the problem at hand. The relationship of this approach with primal integer methods is also addressed. Promising computational results on different proof-of-concept implementations are presented, suggesting that proximity search can be quite effective in quickly refining a given feasible solution. This is particularly true when a sequence of similar MIPs has to be solved as, e.g., in a column-generation setting.",
@article{FischettiMonaci2014,
author={Fischetti, M. and Monaci, M.},
title={Proximity search for 0-1 mixed-integer convex programming},
journal={Journal of Heuristics},
year={2014},
volume={20},
number={6},
pages={709--731},
}
%author={Fischetti, Matteo and Monaci, Michele},
%issn="1572-9397",
%doi="10.1007/s10732-014-9266-x",
%url="http://dx.doi.org/10.1007/s10732-014-9266-x"


@article{GabayMercier1976,
author={D. Gabay and B. Mercier},
title={A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
journal={Computers and Mathematics with Applications},
volume={2},
pages={17--40},
year={1976},
}


%abstract="We present a method for computing lower bounds in the progressive hedging algorithm (PHA) for two-stage and multi-stage stochastic mixed-integer programs. Computing lower bounds in the PHA allows one to assess the quality of the solutions generated by the algorithm contemporaneously. The lower bounds can be computed in any iteration of the algorithm by using dual prices that are calculated during execution of the standard PHA. We report computational results on stochastic unit commitment and stochastic server location problem instances, and explore the relationship between key PHA parameters and the quality of the resulting lower bounds.",
@Article{GadeEtAl2016,
author={Gade, D. and Hackebeil, G. and Ryan, S. M. and Watson, J.-P. and Wets, R. J.-B. and Woodruff, D. L.},
title={Obtaining lower bounds from the progressive hedging algorithm for stochastic mixed-integer programs},
journal={Mathematical Programming},
year={2016},
volume={157},
number={1},
pages={47--67},
}
%author={Gade, Dinakar and Hackebeil, Gabriel and Ryan, Sarah M. and Watson, Jean-Paul and Wets, Roger J.-B. and Woodruff, David L.},
%issn="1436-4646",
%doi="10.1007/s10107-016-1000-z",
%url="http://dx.doi.org/10.1007/s10107-016-1000-z"


@article{GertzWright2003,
author={E.M. Gertz and S.J. Wright}, 
title={Object-oriented software for quadratic programming},
journal={ACM Transactions on Mathematical Software},
volume={29},
number={1},
year={2003},
pages={58--81},
}


@article{GlowinskiMarrocco1975,
author = {R. Glowinski and A. Marrocco},
title={Sur l'approximation, par elements finis
d'ordre un, et la resolution, par penalisation-dualit\'{e}, d'une classe de problems de Dirichlet non lineares},
journal={Revue Fran\c{c}aise d'Automatique, Informatique, et Recherche Op\'{e}rationelle},
volume={9},
pages={41-76},
year ={1975},
}

@article{GS2000,
 author = {Grippo, L. and Sciandrone, M.},
 title = {On the convergence of the block nonlinear {G}auss-{S}eidel method under convex constraints},
 journal = {Operations Research Letters},
 volume = {26},
 number = {3},
 year = {2000},
 pages = {127--136},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
} 

@article{GTURL,
url={http://www2.isye.gatech.edu/~sahmed/siplib/sslp/sslp.html},
note={Last accessed 23 December, 2015},
}

@Inbook{Hamdi1997,
author={Hamdi, A. and Mahey, P. and Dussault, J. P.},
editor={Gritzmann, P. and Horst, R. and Sachs, E. and Tichatschke, R.},
chapter={A New Decomposition Method in Nonconvex Programming via a Separable Augmented Lagrangian},
title={Recent Advances in Optimization: Proceedings of the 8th French-German Conference on Optimization Trier, July 21--26, 1996},
year={1997},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={90--104},
}

@Inbook{HamdiMishra2011,
author={Hamdi, A. and Mishra, S.K.},
editor={Mishra, Shashi Kant},
chapter={Decomposition Methods Based on Augmented Lagrangians: A Survey},
title={Topics in Nonconvex Optimization: Theory and Applications},
year={2011},
publisher={Springer New York},
address={New York, NY},
pages={175--203},
}
%author="Hamdi, Abdelouahed and Mishra, Shashi K.",
%isbn="978-1-4419-9640-4",
%doi="10.1007/978-1-4419-9640-4_11",
%url="http://dx.doi.org/10.1007/978-1-4419-9640-4_11"

@article{HanYuan2012,
year={2012},
journal={Journal of Optimization Theory and Applications},
volume={155},
issue={1},
title={A Note on the Alternating Direction Method of Multipliers},
publisher={Springer US},
author={Han, D. and Yuan, X.},
pages={227-238},
}

@article{HanYuan2013ADMMQP,
author = {Han, D. and Yuan, X.},
title = {Local Linear Convergence of the Alternating Direction Method of Multipliers for Quadratic Programs},
journal = {SIAM Journal on Numerical Analysis},
volume = {51},
number = {6},
pages = {3446-3457},
year = {2013},
}
%doi = {10.1137/120886753},
%
%URL = { 
%        http://dx.doi.org/10.1137/120886753
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/120886753
%    
%}

@article{HareEtAl2016,
author={Hare, W. and Sagastiz\'{a}bal, C. and Solodov, M.},
title={A proximal bundle method for nonsmooth nonconvex functions with inexact information},
journal={ Computational Optimization and Applications},
volume={63},
pages={1--28},
year={2016},
}
 % doi:10.1007/s10589-015-9762-4. Published online
 
%abstract="Letf(x,y) be a function of the vector variablesx ∈Rn andy ∈Rm. The grouped (variable) coordinate minimization (GCM) method for minimizingf consists of alternating exact minimizations in either of the two vector variables, while holding the other fixed at the most recent value. This scheme is known to be locally,q-linearly convergent, and is most useful in certain types of statistical and pattern recognition problems where the necessary coordinate minimizers are available explicitly. In some important cases, the exact minimizer in one of the vector variables is not explicitly available, so that an iterative technique such as Newton's method must be employed. The main result proved here shows that a single iteration of Newton's method solves the coordinate minimization problem sufficiently well to preserve the overall rate of convergence of the GCM sequence.", 
@Article{HathawayBezdek1991,
author="Hathaway, R. J. and Bezdek, J. C.",
title="Grouped coordinate minimization using {N}ewton's method for inexact minimization in one vector coordinate",
journal="Journal of Optimization Theory and Applications",
year="1991",
volume="71",
number="3",
pages="503--516",
}
%issn="1573-2878",
%doi="10.1007/BF00941400",
%url="http://dx.doi.org/10.1007/BF00941400"
 
 
 @article{HeEtAl2002,
year={2002},
journal={Mathematical Programming},
volume={92},
issue={1},
title={A new inexact alternating directions method for monotone variational inequalities},
publisher={Springer-Verlag},
author={B. He and L-Z Liao and D. Han and H. Yang},
pages={103-118},
}
%url={http://dx.doi.org/10.1007/s101070100280},
%keywords={Key words: variational inequality – alternating directions method – inexact method Mathematics Subject Classification (1991): 90C30, 90C33, 65K05},
%doi={10.1007/s101070100280},
%issn={0025-5610},


%abstract="The augmented Lagrangian method (ALM) is a benchmark for solving convex minimization problems with linear constraints. When the objective function of the model under consideration is representable as the sum of some functions without coupled variables, a Jacobian or Gauss--Seidel decomposition is often implemented to decompose the ALM subproblems so that the functions' properties could be used more effectively in algorithmic design. The Gauss--Seidel decomposition of ALM has resulted in the very popular alternating direction method of multipliers (ADMM) for two-block separable convex minimization models and recently it was shown in He et al. (Optimization Online, 2013) that the Jacobian decomposition of ALM is not necessarily convergent. In this paper, we show that if each subproblem of the Jacobian decomposition of ALM is regularized by a proximal term and the proximal coefficient is sufficiently large, the resulting scheme to be called the proximal Jacobian decomposition of ALM, is convergent. We also show that an interesting application of the ADMM in Wang et al. (Pac J Optim, to appear), which reformulates a multiple-block separable convex minimization model as a two-block counterpart first and then applies the original ADMM directly, is closely related to the proximal Jacobian decomposition of ALM. Our analysis is conducted in the variational inequality context and is rooted in a good understanding of the proximal point algorithm.",
@Article{HeEtAl2016,
author={He, B. and Xu, H.-K. and Yuan, X.},
title={On the Proximal Jacobian Decomposition of ALM for Multiple-Block Separable Convex Minimization Problems and Its Relationship to ADMM},
journal={Journal of Scientific Computing},
year={2016},
volume={66},
number={3},
pages={1204--1217},
}
%author={He, Bingsheng and Xu, Hong-Kun and Yuan, Xiaoming},
%issn="1573-7691",
%doi="10.1007/s10915-015-0060-1",
%url="http://dx.doi.org/10.1007/s10915-015-0060-1"

 
@article{Hestenes1969,
author={M. R. Hestenes},
title={Multiplier and gradient methods},
journal={Journal of Optimization Theory and Applications},
volumn={4},
pages={303-320},
year={1969},
}

@article{Hildreth1957,
author={C. Hildreth},
title={A quadratic programming procedure},
journal={Naval Research Logistics Quarterly},
volume={4},
pages={79-85, 361},
year={1957},
}

@book{HirUrrLem1993,
author={Hiriart-Urruty, J.-B. and Lemarechal, C.}, 
year={1993},
title={Convex Analysis and Minimization Algorithms, Vols. I and II}, 
publisher={Springer-Verlag, Berlin and NY},
}



@article{Hohenbalken1977,
year={1977},
journal={Mathematical Programming},
volume={13},
number={1},
title={Simplicial decomposition in nonlinear programming algorithms},
publisher={Springer-Verlag},
author={Von Hohenbalken, B.},
pages={49-68},
}


@article{Holloway1974,
year={1974},
journal={Mathematical Programming},
volume={6},
number={1},
title={An extension of the {F}rank and {W}olfe method of feasible directions},
publisher={Springer-Verlag},
author={Holloway, C.A.},
pages={14--27},
}


%
%A proximal bundle method is presented for minimizing a nonsmooth convex function f. At each iteration, it requires only one approximate evaluation of f and its epsilon-subgradient, and it finds a search direction via quadratic programming. When applied to the Lagrangian decomposition of convex programs, it allows for inexact solutions of decomposed subproblems; yet, increasing their required accuracy automatically, it asymptotically finds both the primal and dual solutions. It is an implementable approximate version of the proximal point algorithm. Some encouraging numerical experience is reported.
@Article{Kiwiel1995,
author="Kiwiel, K. C.",
title="Approximations in proximal bundle methods and decomposition of convex programs",
journal="Journal of Optimization Theory and Applications",
year="1995",
volume="84",
number="3",
pages="529--548",
}
%issn="1573-2878",
%doi="10.1007/BF02191984",
%url="http://dx.doi.org/10.1007/BF02191984"


@article{KiwielEtAl1999,
author = {K.C. Kiwiel and C.H. Rosa and A. {Ruszczy\' nski}},
title = {Proximal Decomposition Via Alternating Linearization},
journal = {SIAM Journal on Optimization},
volume = {9},
number = {3},
pages = {668-689},
year = {1999},
}
%author = {Krzysztof C. Kiwiel and Charles H. Rosa and Andrzej Ruszczynski},
%doi = {10.1137/S1052623495288064},
%
%URL = { 
%        http://dx.doi.org/10.1137/S1052623495288064
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/S1052623495288064
%    
%}


@Article{KiwielLemarechal2009,
author={Kiwiel, K. C. and Lemar{\'e}chal, C.},
title={An inexact bundle variant suited to column generation},
journal={Mathematical Programming},
year={2009},
volume={118},
number={1},
pages={177--206},
}



@article{KontogiorgisMeyer1998,
year={1998},
issn={0025-5610},
journal={Mathematical Programming},
volume={83},
issue={1-3},
doi={10.1007/BF02680549},
title={A variable-penalty alternating directions method for convex optimization},
url={http://dx.doi.org/10.1007/BF02680549},
publisher={Springer-Verlag},
keywords={Alternating direction methods; Decomposition; Parallel computing; Block angular programs},
author={S. Kontogiorgis, and R. R. Meyer},
pages={29-53},
language={English},
}


%abstract="In this paper, we propose a new decomposition method for solving convex programming problems with separable structure. The proposed method is based on the decomposition method proposed by Chen and Teboulle and the nonlinear proximal point algorithm using the Bregman function. An advantage of the proposed method is that, by a suitable choice of the Bregman function, each subproblem becomes essentially the unconstrained minimization of a finite-valued convex function. Under appropriate assumptions, the method is globally convergent to a solution of the problem.",
@Article{KyonoFukushima2000,
author={Kyono, M. and Fukushima, M.},
title={Nonlinear Proximal Decomposition Method for Convex Programming},
journal={Journal of Optimization Theory and Applications},
volume={106},
number={2},
pages={357--372},
}
%issn="1573-2878",
%doi="10.1023/A:1004655531273",
%url="http://dx.doi.org/10.1023/A:1004655531273"


@Inbook{LemarechalWolfe1975,
author="Lemar\'{e}chal, C.",
editor="Balinski, M. L. and Wolfe, Philip",
title="An extension of {D}avidon methods to non differentiable problems",
bookTitle="Nondifferentiable Optimization",
year="1975",
publisher="Springer Berlin Heidelberg",
pages="95--109",
}


@article{LiEtAl2008,
author={Li Y and Lu Z. and Michalek J.J.},
title={Diagonal Quadratic Approximation for Parallelization of Analytical Target Cascading},
journal={Journal of Mechanical Design},
year={2008},
volume={130},
number={5},
}

@article{LinEtAl2014,
author={X. Lin and M. Pham and A. Ruszczy\'nski},
title={Alternating linearization for structured regularization problems},
journal={Journal of Machine Learning Research},
volume={15},
pages={3447--3481},
year={2014},
}





@article{LiuEtAl2015FWAL,
author={Y. Liu and X. Wang and X. Liu and S. Ma},
title={A Scalable Frank-Wolfe based Augmented Lagrangian Method for Linearly Constrained Composite Convex Programming},
journal={Optimization Online},
url={http://www.optimization-online.org/DB\_FILE/2015/07/5033.pdf},
note={last accessed 12 August 2015},
year={2015},
}


@inproceedings{LubinEtAl2011,
author={M. Lubin and C.G. Petra and M. Anitescu and V. Zavala}, 
title={Scalable stochastic optimization of complex energy systems}, 
booktitle={Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis}, 
publisher={ACM, Seattle, WA}, 
year={2011}, 
pages={64:1–64:10},
}


%abstract = "For stochastic mixed-integer programs, we revisit the dual decomposition algorithm of Carøe and Schultz from a computational perspective with the aim of its parallelization. We address an important bottleneck of parallel execution by identifying a formulation that permits the parallel solution of the master program by using structure-exploiting interior-point solvers. Our results demonstrate the potential for parallel speedup and the importance of regularization (stabilization) in the dual optimization. Load imbalance is identified as a remaining barrier to parallel scalability. "
@article{LubinEtAl2013,
title = {On parallelizing dual decomposition in stochastic integer programming},
author = {M. Lubin and K. Martin and C.G. Petra and B. Sand{\i}k\c{c}{\i}},
journal = {Operations Research Letters},
volume = {41},
number = {3},
pages = {252--258},
year = {2013},
}
%note = "",
%issn = "0167-6377",
%doi = "http://dx.doi.org/10.1016/j.orl.2013.02.003",
%url = "http://www.sciencedirect.com/science/article/pii/S0167637713000242",
%keywords = "Stochastic programming",
%keywords = "Mixed-integer programming",
%keywords = "Column generation",
%keywords = "Dual decomposition",
%keywords = "Parallel computing",
%keywords = "Bundle methods ",


@article{MaheyEtAl1995,
author = {Philippe Mahey and Said Oualibouch and Pham Dinh Tao},
title = {Proximal Decomposition on the Graph of a Maximal Monotone Operator},
journal = {SIAM Journal on Optimization},
volume = {5},
number = {2},
pages = {454--466},
year = {1995},
}
%doi = {10.1137/0805023},
%
%URL = { 
%        http://dx.doi.org/10.1137/0805023
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/0805023
%    
%}


@article{MulveyRuszczynski1992,
title = {A diagonal quadratic approximation method for large scale linear programs},
journal = {Operations Research Letters},
volume = {12},
number = {4},
pages = {205 - 215},
year = {1992},
author = {J.M. Mulvey and A. Ruszczy\'nski},
}
%author = {John M. Mulvey and Andrzej Ruszczy\'nski},
%note = "",
%issn = "0167-6377",
%doi = "http://dx.doi.org/10.1016/0167-6377(92)90046-6",
%url = "http://www.sciencedirect.com/science/article/pii/0167637792900466",
%keywords = "linear programming",
%keywords = "stochastic programming",
%keywords = "decomposition "


@manual{NCIURL,
title={NCI Website},
organization={National Computing Infrastructure (NCI)},
url={http://www.nci.org.au},
note={Last accessed 19 November 2016},
}
%note={http://www.nci.org.au, last accessed 20 April, 2016},

@article{NgEtAl2011,
author = {Ng, M. and Wang, F. and Yuan, X.},
title = {Inexact Alternating Direction Methods for Image Recovery},
journal = {SIAM Journal on Scientific Computing},
volume = {33},
number = {4},
pages = {1643-1668},
year = {2011},
}
%doi = {10.1137/100807697},
%URL = {http://epubs.siam.org/doi/abs/10.1137/100807697},
%eprint = {http://epubs.siam.org/doi/pdf/10.1137/100807697},

@phdthesis{NtaimoPhD2004,
 author = {Ntaimo, Lewis},
 title = {Decomposition Algorithms for Stochastic Combinatorial Optimization: Computational Experiments and Extensions},
 year = {2004},
 publisher = {The University of Arizona},
} 


@article{OliveiraEtAl2011,
author = {W. Oliveira and C. Sagastizábal and S. Scheimberg},
title = {Inexact Bundle Methods for Two-Stage Stochastic Programming},
journal = {SIAM Journal on Optimization},
volume = {21},
number = {2},
pages = {517--544},
year = {2011},
}
%author = {Welington Oliveira and Claudia Sagastizábal and Susana Scheimberg},
%doi = {10.1137/100808289},
%
%URL = { 
%        http://dx.doi.org/10.1137/100808289
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/100808289
%    
%}
@article{OliveiraSagastizabal2014,
   title = {{Bundle Methods in the XXIst Century: A Bird's-eye view}},
   journal = {{Pesquisa Operacional}},
   author={de Oliveira, W. and Sagastiz\'{a}bal, C.},
   volume = {34},
   year = {2014},
   month = {12},
   pages = {647--670},
}
%   ISSN = {0101-7438},
%   language = {en},
%   URL = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0101-74382014000300647&nrm=iso},
%    crossref = {10.1590/0101-7438.2014.034.03.0647},
%   author={Oliveira, Welington de and Sagastiz\'{a}bal, Claudia},
%   publisher = {scielo},


@Article{OliveiraEtAl2014,
author={de Oliveira, W. and Sagastiz{\'a}bal, C. and Lemar{\'e}chal, C.},
title={Convex proximal bundle methods in depth: a unified analysis for inexact oracles},
journal={Mathematical Programming},
year={2014},
volume={148},
number={1},
pages={241--277},
}


@book{Poljak1987,
author={B. T. Poljak},
title={Introduction to Optimization},
publisher={Optimization Software Inc., New York},
year={1987},
}


@incollection{Powell1969,
author={M. J. D. Powell},
title={A method for nonlinear constraints in minimization problems},
editor={R. Fletcher},
booktitle={Optimization},
publisher={New York: Academic Press},
year={1969},
}

@article{Rockafellar1973MM,
year={1973},
journal={Journal of Optimization Theory and Applications},
volume={12},
number={6},
title={The multiplier method of {H}estenes and {P}owell applied to convex programming},
publisher={Kluwer Academic Publishers-Plenum Publishers},
author={Rockafellar, R.T.},
pages={555-562},
}
%issn={0022-3239},
%doi={10.1007/BF00934777},
%url={http://dx.doi.org/10.1007/BF00934777},
%language={English}

@article{Rockafellar1976MO,
author = {R.T. Rockafellar},
title = {Monotone Operators and the Proximal Point Algorithm},
journal = {SIAM Journal on Control and Optimization},
volume = {14},
number = {5},
pages = {877--898},
year = {1976},
}
%author = {R. Tyrrell Rockafellar},
%doi = {10.1137/0314056},
%
%URL = { 
%        http://dx.doi.org/10.1137/0314056
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/0314056
%    
%}


@article{Rockafellar1976ALMethod,
     title = {Augmented {L}agrangians and Applications of the Proximal Point Algorithm in Convex Programming},
     author = {Rockafellar, R. T.},
     journal = {Mathematics of Operations Research},
     volume = {1},
     number = {2},
     pages = {97--116},
     year = {1976},
     publisher = {INFORMS},
}
%     url = {http://www.jstor.org/stable/3689277},
%     ISSN = {0364765X},
%     abstract = {The theory of the proximal point algorithm for maximal monotone operators is applied to three algorithms for solving convex programs, one of which has not previously been formulated. Rate-of-convergence results for the "method of multipliers," of the strong sort already known, are derived in a generalized form relevant also to problems beyond the compass of the standard second-order conditions for optimality. The new algorithm, the "proximal method of multipliers," is shown to have much the same convergence properties, but with some potential advantages.},
%     language = {English},
%     copyright = {Copyright © 1976 INFORMS},


@article{Rockafellar1974,
author={R.T. Rockafellar},
title={Augmented {L}agrange multiplier functions and duality in nonconvex programming},
journal={S{I}{A}{M} Journal on Control},
volume={12},
pages={268-285},
}

@book{Rockafellar1970,
author={R.T. Rockafellar},
title={Convex Analysis},
publisher={Princeton University Press},
year={1970},
}


@article{Ruszczynski1986,
author={Ruszczy{\'{n}}ski, A.},
title={A regularized decomposition method for minimizing a sum of polyhedral functions},
journal={Mathematical Programming},
volume={35},
number={3},
pages={309--333},
year={1986},
}

@incollection{Ruszczynski1988,
year={1988},
author={A. Ruszczy\'nski},
booktitle={Theory, Software and Testing Examples in Decision Support Systems},
editor={A. Lewandowski and A. Wierzbicki},
title={Regularized decomposition and augmented {L}agrangian decomposition for angular linear programming problems},
publisher={Springer Berlin},
pages={p. 71},
}

@article{Ruszczynski1989,
title = {An augmented Lagrangian decomposition method for block diagonal linear programming problems},
journal = {Operations Research Letters},
volume = {8},
number = {5},
pages = {287--294},
year = {1989},
author = {A. Ruszczy\'{n}ski},
}
%author = {Andrzej Ruszczy\'{n}ski},
%note = "",
%issn = "0167-6377",
%doi = "http://dx.doi.org/10.1016/0167-6377(89)90055-2",
%url = "http://www.sciencedirect.com/science/article/pii/0167637789900552",
%keywords = "linear programming",
%keywords = "decomposition",
%keywords = "augmented Lagrangians ",
%abstract = "A new decomposition method for large linear programming problems of angular structure is proposed. It combines the ideas of augmented Lagrangians, simplicial decomposition, column generation and dual ascent. Finite termination of the method is proved. "

@article{Ruszczynski1995,
 author = {A. Ruszczy\'nski},
 journal = {Mathematics of Operations Research},
 number = {3},
 pages = {634--656},
 publisher = {INFORMS},
 title = {On Convergence of an Augmented {L}agrangian Decomposition Method for Sparse Convex Optimization},
 volume = {20},
 year = {1995}
}
%author = {Andrzej Ruszczyński},
% ISSN = {0364765X, 15265471},
% URL = {http://www.jstor.org/stable/3690175},
% abstract = {A decomposition method for large-scale convex optimization problems with block-angular structure and many linking constraints is analysed. The method is based on a separable approximation of the augmented Lagrangian function. Weak global convergence of the method is proved and speed of convergence analysed. It is shown that convergence properties of the method are heavily dependent on sparsity of the linking constraints. Application to large-scale linear programming and stochastic programming is discussed.},

@article{Ruszczynski1997,
 author = {Ruszczy\'{n}ski, A.},
 title = {Decomposition Methods in Stochastic Programming},
 journal = {Math. Program.},
 volume = {79},
 number = {1-3},
 year = {1997},
 pages = {333--353},
} 

@incollection{RuszczynskiShapiro2003,
title = {Optimality and Duality in Stochastic Programming},
editor = {A. Shapiro and A. Ruszczy\'{n}ski},
booktitle = {Stochastic Programming},
publisher = {Elsevier},
year = {2003},
volume = {10},
pages = {65 - 139},
series = {Handbooks in Operations Research and Management Science},
author = {A. Ruszczy\'{n}ski and A. Shapiro},
}
%author = {Andrzej Ruszczy\'{n}ski and Alexander Shapiro},
%issn = "0927-0507",
%doi = "http://dx.doi.org/10.1016/S0927-0507(03)10002-3",
%url = "http://www.sciencedirect.com/science/article/pii/S0927050703100023",
%keywords = "Expected value function",
%keywords = "two stage stochastic programming",
%keywords = "multistage stochastic programming",
%keywords = "optimality conditions",
%keywords = "duality "

@book{Ruszczynski2006,
author = {A. {Ruszczy\' nski}},
title = {Nonlinear Optimization},
publisher = {Princeton University Press},
year = {2006},
}

@article{Sagastizabal2013,
author={Sagastiz{\'a}bal, C.},
title={Composite proximal bundle method},
journal={Mathematical Programming},
year={2013},
volume={140},
number={1},
pages={189--233},
}
%author={Sagastiz{\'a}bal, Claudia},
%abstract="We consider minimization of nonsmooth functions which can be represented as the composition of a positively homogeneous convex function and a smooth mapping. This is a sufficiently rich class that includes max-functions, largest eigenvalue functions, and norm-1 regularized functions. The bundle method uses an oracle that is able to compute separately the function and subgradient information for the convex function, and the function and derivatives for the smooth mapping. With this information, it is possible to solve approximately certain proximal linearized subproblems in which the smooth mapping is replaced by its Taylor-series linearization around the current serious step. Our numerical results show the good performance of the Composite Bundle method for a large class of problems. ",
%issn="1436-4646",
%doi="10.1007/s10107-012-0600-5",
%url="http://dx.doi.org/10.1007/s10107-012-0600-5"


%Standard citation for the subgradient method
@book{Shor1985,
author={N.Z. Shor},
title={Minimization Methods for Non-differentiable Functions},
publisher={Springer-Verlag, New York},
year={1985},
}



%abstract = "The aim of the paper is to present a new dual-type decomposition algorithm for large-scale nonconvex optimization problems of general separable structure. After reformulation of the augmented Lagrange function and introduction of auxiliary variables, called approximation points, separable local optimization problems are created. The lower level of the method consists of a few optimization runs of these problems for subsequently improved approximation points, whereas standard updating of Lagrange multipliers constitutes the highest level. Simple rules for adjusting the approximation points and the Lagrange multipliers are given and thoroughly analysed. Applicability conditions and example numerical results indicate that the presented algorithm eliminates, to a great extent, drawbacks of the previous approaches. "
@article{Tatjewski1989,
title = "New dual-type decomposition algorithm for nonconvex separable optimization problems ",
journal = "Automatica ",
volume = "25",
number = "2",
pages = "233--242",
year = "1989",
author = "P. Tatjewski",
}
%keywords = "Large-scale systems",
%keywords = "decomposition",
%keywords = "nonlinear programming",
%keywords = "augmented Lagrange functions ",
%note = "",
%issn = "0005-1098",
%doi = "http://dx.doi.org/10.1016/0005-1098(89)90076-9",
%url = "http://www.sciencedirect.com/science/article/pii/0005109889900769",




@Article{TaoYuan2012,
author={Tao, M. and Yuan, X.},
title={An inexact parallel splitting augmented {L}agrangian method for monotone variational inequalities with separable structures},
journal={Computational Optimization and Applications},
year={2012},
volume={52},
number={2},
pages={439--461},
}
%author={Tao, Min and Yuan, Xiaoming},
%abstract="Splitting methods have been extensively studied in the context of convex programming and variational inequalities with separable structures. Recently, a parallel splitting method based on the augmented Lagrangian method (abbreviated as PSALM) was proposed in He (Comput. Optim. Appl. 42:195--212, 2009) for solving variational inequalities with separable structures. In this paper, we propose the inexact version of the PSALM approach, which solves the resulting subproblems of PSALM approximately by an inexact proximal point method. For the inexact PSALM, the resulting proximal subproblems have closed-form solutions when the proximal parameters and inexact terms are chosen appropriately. We show the efficiency of the inexact PSALM numerically by some preliminary numerical experiments.",
%issn="1573-2894",
%doi="10.1007/s10589-011-9417-z",
%url="http://dx.doi.org/10.1007/s10589-011-9417-z"

@article{TappendenEtAl2015,
author={R. Tappenden and P. Richt\'{a}rik and B. B{\"u}ke},
title={Separable approximations and decomposition methods for the augmented {L}agrangian},
journal={Optimization Methods \& Software},
volume={30},
number={3},
pages={643--668},
year={2015},
}


%abstract = "The aim of the paper is to present a new dual-type decomposition algorithm for large-scale nonconvex optimization problems of general separable structure. After reformulation of the augmented Lagrange function and introduction of auxiliary variables, called approximation points, separable local optimization problems are created. The lower level of the method consists of a few optimization runs of these problems for subsequently improved approximation points, whereas standard updating of Lagrange multipliers constitutes the highest level. Simple rules for adjusting the approximation points and the Lagrange multipliers are given and thoroughly analysed. Applicability conditions and example numerical results indicate that the presented algorithm eliminates, to a great extent, drawbacks of the previous approaches. "
@article{Tatjewski1989,
title = {New dual-type decomposition algorithm for nonconvex separable optimization problems},
journal = {Automatica},
volume = {25},
number = {2},
pages = {233--242},
year = {1989},
author = {P. Tatjewski},
}
%keywords = "Large-scale systems",
%keywords = "decomposition",
%keywords = "nonlinear programming",
%keywords = "augmented Lagrange functions ",
%issn = "0005-1098",
%doi = "http://dx.doi.org/10.1016/0005-1098(89)90076-9",
%url = "http://www.sciencedirect.com/science/article/pii/0005109889900769",
%note = "",


%abstract="A two-level decomposition method for nonconvex separable optimization problems with additional local constraints of general inequality type is presented and thoroughly analyzed in the paper. The method is of primal-dual type, based on an augmentation of the Lagrange function. Previous methods of this type were in fact three-level, with adjustment of the Lagrange multipliers at one of the levels. This level is eliminated in the present approach by replacing the multipliers by a formula depending only on primal variables and Kuhn-Tucker multipliers for the local constraints. The primal variables and the Kuhn-Tucker multipliers are together the higher-level variables, which are updated simultaneously. Algorithms for this updating are proposed in the paper, together with their convergence analysis, which gives also indications on how to choose penalty coefficients of the augmented Lagrangian. Finally, numerical examples are presented.",
@article{TatjewskiEngelmann1990,
author={Tatjewski, P. and Engelmann, B.},
title={Two-level primal-dual decomposition technique for large-scale nonconvex optimization problems with constraints},
journal={Journal of Optimization Theory and Applications},
volume={64},
number={1},
pages={183--205},
year={1990},
}
%issn="1573-2878",
%doi="10.1007/BF00940031",
%url="http://dx.doi.org/10.1007/BF00940031"




@article{Tseng1997,
author = {P. Tseng},
title = {Alternating Projection-Proximal Methods for Convex Programming and Variational Inequalities},
journal = {SIAM Journal on Optimization},
volume = {7},
number = {4},
pages = {951-965},
year = {1997},
}
%author = {Paul Tseng},
%doi = {10.1137/S1052623495279797},
%
%URL = { 
%        http://dx.doi.org/10.1137/S1052623495279797
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1137/S1052623495279797
%    
%}

@article{Tseng2001,
year={2001},
journal={Journal of Optimization Theory and Applications},
volume={109},
issue={3},
title={Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization},
publisher={Kluwer Academic Publishers-Plenum Publishers},
author={Tseng, P.},
pages={475-494},
}
%issn={0022-3239},
%doi={10.1023/A:1017501703105},
%url={http://dx.doi.org/10.1023/A%3A1017501703105},
%keywords={block coordinate descent; nondifferentiable minimization; stationary point; Gauss–Seidel method; convergence; quasiconvex functions; pseudoconvex functions},
%language={English},

@article{TsengYun2009,
year={2009},
journal={Mathematical Programming},
volume={117},
issue={1-2},
title={A coordinate gradient descent method for nonsmooth separable minimization},
publisher={Springer-Verlag},
author={Tseng, P. and Yun, S.},
pages={387-423},
}
%doi={10.1007/s10107-007-0170-0},
%url={http://dx.doi.org/10.1007/s10107-007-0170-0},
%keywords={Error bound; Global convergence; Linear convergence rate; Nonsmooth optimization; Coordinate descent; 49M27; 49M37; 65K05; 90C06; 90C25; 90C26; 90C30; 90C55},
%language={English},
%issn={0025-5610},

@article{Warga1963,
author={J. Warga},
title={Minimizing Certain Convex Functions},
journal={SIAM Journal on Applied Mathematics},
volume={11},
pages={588--593},
year={1963},
}

@Article{ZhaoLuh2002,
author={Zhao, X. and Luh, P.B.},
title={New Bundle Methods for Solving {L}agrangian Relaxation Dual Problems},
journal={Journal of Optimization Theory and Applications},
volume={113},
number={2},
pages={373--397},
year={2002},
}
%abstract="Bundle methods have been used frequently to solve nonsmooth optimization problems. In these methods, subgradient directions from past iterations are accumulated in a bundle, and a trial direction is obtained by performing quadratic programming based on the information contained in the bundle. A line search is then performed along the trial direction, generating a serious step if the function value is improved by ∈ or a null step otherwise. Bundle methods have been used to maximize the nonsmooth dual function in Lagrangian relaxation for integer optimization problems, where the subgradients are obtained by minimizing the performance index of the relaxed problem. This paper improves bundle methods by making good use of near-minimum solutions that are obtained while solving the relaxed problem. The bundle information is thus enriched, leading to better search directions and less number of null steps. Furthermore, a simplified bundle method is developed, where a fuzzy rule is used to combine linearly directions from near-minimum solutions, replacing quadratic programming and line search. When the simplified bundle method is specialized to an important class of problems where the relaxed problem can be solved by using dynamic programming, fuzzy dynamic programming is developed to obtain efficiently near-optimal solutions and their weights for the linear combination. This method is then applied to job shop scheduling problems, leading to better performance than previously reported in the literature.",
%issn="1573-2878",
%doi="10.1023/A:1014839227049",
%url="http://dx.doi.org/10.1023/A:1014839227049"
