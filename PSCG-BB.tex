%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
%\usepackage{graphicx}

\usepackage{amssymb,graphicx,amscd,setspace,verbatim,amsmath,color,algpseudocode,algorithm,mathabx,float}
%\newtheorem{theorem}{Theorem}
%\newtheorem{conjecture}{Conjecture}
%\newtheorem{lemma}{Lemma}
%\newtheorem{remark}{Remark}
%\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}

\newcommand{\braces}[1]{\left\{ #1 \right \}}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\dvar}{\mu}
\newcommand{\penC}{\sigma}
\newcommand{\norm}[1]{\left \| #1 \right \|}
\newcommand{\conv}{\operatorname{conv}}
\newcommand{\cone}{\text{cone}}
\newcommand{\linear}{\text{line}}
\newcommand{\eNum}{\text{e}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Ind}{\text{Ind}}
\newcommand{\proj}{\text{P}}

\newcommand{\colorA}[1]{\textcolor{red}{#1}}
\newcommand{\colorB}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\newcommand{\scenI}{s}
\newcommand{\nS}{s}
\newcommand{\scenSet}{\Omega}
\newcommand{\slack}{\gamma}
\newcommand{\x}{x}
\newcommand{\y}{y}
\newcommand{\X}{X}
\newcommand{\Y}{Y}
\newcommand{\dualFnc}{q}
\newcommand{\iLoopInd}{t}

\newcommand{\h}{h}
\newcommand{\ttt}{t}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\s}{s}
\newcommand{\SC}{S}
\newcommand{\SSS}{\mathcal{S}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

%\newcommand{\TheTitle}{A parallelizable augmented Lagrangian method applied to computing Lagrangian bounds of large-scale non-convex-constrained optimization problems} 
\newcommand{\TheTitle}{...}%A parallelizable augmented Lagrangian method applied to large-scale non-convex-constrained optimization problems} 
\newcommand{\TheAuthors}{...}%Brian Dandurand, Natashia Boland, Jeffrey Christiansen, Andrew Eberhard, Fabricio Oliveira}

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

%\title{{\TheTitle}\thanks{This work was supported by the Australian Research Council (ARC) grant ARC DP140100985.}}
\title{{\TheTitle}\thanks{}}
%\title{Insert your title here%\thanks{Grants or other notes
%%about the article that should go on the front page should be
%%placed here. General acknowledgments should be placed at the end of the article.}
%}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{A parallelizable augmented Lagrangian method...}        % if too long for running head

%\author{First Author         \and
%        Second Author %etc.
%}


\author{  ...
% Natashia Boland
%    \and
%  Jeffrey Christiansen
%  \and
%  Brian Dandurand
%  \and
%  Andrew Eberhard
%  \and
%  Fabricio Oliveira
}
    


%\authorrunning{Short form of author list} % if too long for running head

\institute{
%             \emph{Present address:} of F. Author  %  if needed
%N. Boland \at Georgia Institute of Technology, Atlanta, , USA 
%\and
%J. Christiansen \at RMIT University, Melbourne, Victoria, Australia 
%\and
%B. Dandurand \at RMIT University, Melbourne, Victoria, Australia 
%\and
%A. Eberhard \at RMIT University, Melbourne, Victoria, Australia \\
%              Tel.: +61-3-9925-2616\\
%              Fax: +61-3-9925-1748\\
%              \email{andy.eberhard@rmit.edu.au}           %  \\
%\and
%F. Oliveira \at RMIT University, Melbourne, Victoria, Australia 
}

%\institute{F. Author \at
%              first address \\
%              Tel.: +123-45-678910\\
%              Fax: +123-45-678910\\
%              \email{fauthor@example.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
%           \and
%           S. Author \at
%              second address
%}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
%We contribute improvements to a Lagrangian dual solution approach applied to large-scale optimization problems whose objective functions are convex,  continuously differentiable and possibly nonlinear, while the non-relaxed constraint set is compact but not necessarily convex. Such problems arise, for example, in the split-variable deterministic reformulation of stochastic mixed-integer optimization problems. The dual solution approach needs to address the nonconvexity of the non-relaxed constraint set while being efficiently implementable in parallel.
%We adapt the augmented Lagrangian method framework to address the presence of nonconvexity in the non-relaxed constraint set and the need for efficient parallelization. The development of our approach is most naturally compared with the development of proximal bundle methods and especially with their use of serious step conditions. However, deviations from these developments allow for an improvement in efficiency with which parallelization can be utilized. 
%Pivotal in our modification to the augmented Lagrangian method is the use of an integration of approaches based on the simplicial decomposition method (SDM) and the nonlinear block Gauss-Seidel (GS) method. 
%An adaptation of a serious step condition associated with proximal bundle methods allows for the approximation tolerance to be automatically adjusted. 
%Under mild conditions  optimal dual convergence is proven, and we report computational results on test instances from the stochastic optimization literature.
%We demonstrate improvement in parallel speedup over a baseline parallel approach.

%\keywords{First keyword \and Second keyword \and More}
\keywords{augmented Lagrangian method \and proximal bundle method \and nonlinear block Gauss-Seidel method \and simplicial decomposition method \and parallel computing}
%proximal simplicial decomposition method, dual decomposition, mixed-integer optimization, large scale optimization
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\subclass{90-08, 90C06, 90C11, 90C15, 90C25, 90C26, 90C30, 90C46}
%90-08, Computational methods 
%90C06, Large-scale problems
%90C11, Mixed-integer programming
%90C15, Stochastic programming
%90C25, Convex programming
%90C26, Nonconvex programming
%90C30, Nonlinear programming
%90C46 Optimality conditions, duality
\end{abstract}




\section{Introduction and Background}
%Motivated by applications in stochastic mixed-integer optimization, 
%which 1) benefits from the regularization of proximal point methods; and 2) is efficiently parallelizable in the sense of maximizing the computational work that can be parallelized, the memory usage that can be distributed, and minimizing the amount of parallel communication. %This sense of efficiency in parallelization is correlated with scalability, where increased parallel efficiency leads to improved benefit of using more processors.  
%Within the setting of proximal point methods, the nonconvexity of the constraint set brings up the issue of practical implementability, and the proximal term is the main source of impediment in the efficient parallelization.
%%while the issue of efficient parallelization is addressed by the integration of a nonlinear block Gauss-Seidel approach with the proximal point methodology while maintaining practical implementability. %We then specialize this approach to an application in stochastic mixed-integer programming for computing Lagrangian dual bounds efficiently and in parallel.
The problem of interest has the form
\begin{equation}\label{P1}
f^*:=\min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ c^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in K_i,  x_i=z \text{ for }i=1,\dots,m,\; z \in \mathbb{R}^{n_1}},
\end{equation}
where $K$ is a bounded polyhedral set intersected with mixed-integrality constraints. 
%where The vector $x \in X$ of decision variables  is derived from the original decisions associated with a problem, while the vector $z \in Z$ of auxiliary variables are introduced to effect a decomposable structure in~\eqref{P1}. The block diagonal components of $Q$ are denoted $Q_i \in \mathbb{R}^{q_i \times n_i}$, $i=1,\dots,m$.
%Once the constraint $Qx=z$ is relaxed, the decomposable structure of~\eqref{P1} becomes evident due to the following structural assumptions:
%\begin{enumerate}
%\item $X=X_1 \times \cdots \times X_m \subset \mathbb{R}^{n_1} \times \cdots \times \mathbb{R}^{n_m}$ with $\sum_{i=1}^m n_i = n$, so that we may write $x=(x_1,\dots,x_m)$,
%\item $Q$ is a block diagonal matrix with block diagonal components $Q_i \in \mathbb{R}^{q_i \times n_i}$,
%\item $f(x)=\sum_{i=1}^m f_i(x_i)$ where $f_i : \mathbb{R}^{n_i} \mapsto \mathbb{R}$ are convex and continuously differentiable for $i=1,\dots,m$.
%\end{enumerate} 
%Problem~\eqref{P1} is general enough to subsume, for example, the split-variable deterministic reformulation of a stochastic optimization problem with potentially multiple stages, as defined, for example, in~\cite{BirgeLouveaux2011}, while it can also model the case where $f$ is nonlinear (and convex) and/or $X$ is any compact (but not necessarily convex) set.

We develop a branch-and-bound approach to solving~\eqref{P1} based on solving Lagrangian relaxations due to relaxing $Qx=z$.
At each node, we use the recently developed algorithm SDM-GS-ALM to solve two equivalent characterizations of the Lagrangian dual problem.
The Lagrangian dual function is defined 
$$\phi(\omega) := \min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ c^\top x_i + \omega_i^\top(x_i-z) + d_i^\top y_i \right] : (x_i,y_i) \in K_i,\text{ for }i=1,\dots,m,\; z \in \mathbb{R}^{n_1}}.$$ 
Under the dual feasibility condition $\sum_{i=1}^m p_i \omega_i = 0$, we have 
$$\phi(\omega) = \min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ \left(c+\omega_i\right)^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in K_i, \text{ for }i=1,\dots,m,\;z \in \mathbb{R}^{n_1}}.$$ 
The set of solutions verifying the value of $\phi(\omega)$ is denoted
$$
K^*(\omega):=\argmin_{x,y} \braces{\sum_{i=1}^m p_i \left[ \left(c+\omega_i\right)^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in K_i, \text{ for }i=1,\dots,m,\; z \in \mathbb{R}^{n_1}}.
$$
%When $f$ is not linear, we also need to refer to the convexified Lagrangian dual funciton  
%$\phi^C(\omega) := \min_{x,z} \braces{f(x) + \omega^\top (Qx-z) : x \in \conv(X), z \in Z},$ 
%for which analogously,
%we have 
%\begin{equation}\label{PhiC}
%\phi^C(\omega) = \min_x \braces{f(x) + \omega^\top Qx : x \in \conv(X)}
%\end{equation} 
%when $\omega \in Z^\perp$. 
%Optimal solutions to problem~\eqref{PhiC} will be referred to generically as $\widehat{x}(\omega)$. 
%\begin{remark}
%Under the assumption that $\conv(X)$ is not known beforehand by any characterization, direct evaluation of $\phi^C$ or any of its subgradients at any $\omega \in Z^\perp$ is not possible. 
%This dual function is only treated indirectly in the following algorithms.
%%Nevertheless, the necessity of referring to  $\phi^C$  will become clear later in this section. %Also, problem~\eqref{P1ConvB} is the primal characterization to~\eqref{P2C}.
%\end{remark}



The Lagrangian dual problem is:
\begin{equation}\label{D1}
\phi^*:=\max_{\omega \in \Omega} \phi(\omega).
\end{equation}
Its well-known primal characterization takes the form
\begin{equation}\label{D2}
f_C^*:=\min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ c^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in \conv(K_i),  x_i=z \text{ for }i=1,\dots,m,\; z \in \mathbb{R}^{n_1}}.
\end{equation}
It is well-known [under our standing assumptions] that $f_C^*=\phi^*$, although in general, $\phi^* \le f^*$, with $\phi^* < f^*$ being typical with the presence of mixed-integrality constraints on $x$ and $y$.


%It is straightforward from the definitions that $\phi^C(\omega) \le \phi(\omega)$ for all dual feasible $\omega \in Z^\perp$. 
%In the case when $f$ is linear, we have $\phi^C(\omega) = \phi(\omega)$ for all $\omega \in Z^\perp$ and so $\phi^*=\phi_C^*$.
%%and the primal characterizations~\eqref{P1Conv} and~\eqref{P1ConvB} of the respective dual problems are equivalent.
%But in the general case where $f$ is nonlinear, the dual~\eqref{P2C} can be ``weaker'' than~\eqref{P2}, where $\phi_C^* < \phi^*$ can occur,
%which we see in the following example.
%%, andthe primal characterizations ~\eqref{P1Conv} and~\eqref{P1ConvB} of the respective dual problems are not equivalent. 
%Let $f : \mathbb{R}^2 \mapsto \mathbb{R}$ be defined by $f(x)=(x_1-0.5)^2+(x_2-0.5)^2$, $X=\braces{0,1} \times \braces{0,1}$, and let $Qx=z$ be defined to model the constraints $x_1 -z_1 = 0$ and $x_2 - z_2 = 0$ where $Z = \braces{(z_1,z_2) : z_1=z_2} \subset \mathbb{R}^2$. We see trivially that $\phi_C^* = 0$, which is verified with the saddle point $x_1^*=x_2^*=z_1^*=z_2^*=0.5$ and $\omega^*=(0,0)$. However,
%$\phi^* = 0.5$, which is verified with either of the saddle points $x_1^*=x_2^*=z_1^*=z_2^*=0$ and $\omega^*=(0,0)$, or $x_1^*=x_2^*=z_1^*=z_2^*=1$ and $\omega^*=(0,0)$. Thus,  $\phi_C^* < \phi^*$.

Dual optimal solutions to~\eqref{D1}, when they exist, are generically denoted by $\omega^*$, and and the set of corresponding optimal primal solutions verifying the value $\phi(\omega^*)$ is denoted by $K^*(\omega^*)$. The optimal solutions to the second characterization~\eqref{D2}, when they exist, are denoted $(x^*,y^*,z^*)$. We shall denote the set $Z:=\braces{(x,y) \in \conv(K) : \exists \; z \in \mathbb{R}^n \; \text{s.t.} \; x_i=z\; \text{for }i=1,\dots,m}$. 
%$$K_Z := \braces{ (x,y) \in K : \exists \; z \in \mathbb{R}^n \; \text{s.t.} \; x_i=z\; \text{for }i=1,\dots,m}.$$ 
%It is straightforward to show that $\conv(X_Z) = \braces{x \in \conv(X)  : \exists \; z \in Z \; \text{s.t.} \; Qx=z}$ under the assumption that $Z$ is convex.

We shall use both characterizations~\eqref{D1} and~\eqref{D2} to inform different ways of branching on variables and branching values. 
\begin{enumerate}
\item Viewing the node subproblems as instances of problem~\eqref{D2}, we have optimal solutions $(x^*,y^*,z^*)$ to problem~\eqref{D2} that satisfy 
$$(x^*,y^*,z^*) \in \braces{(x,y,z) : (x,y) \in \conv(K), x_i=z,\; i=1,\dots,m},$$ 
but $(x^*,y^*) \not\in K$ in the case of strict duality gap $\phi_C^* < f^*$. Thus we would project $(x^*,y^*)$ onto $K$, denoting the projection by $\proj_K((x^*,y^*))$. (Given that $K$ is defined by polyhedral and mixed-integer constraints, this projection can just take the form of rounding the components of $(x,y)$ with integer restrictions to their nearest integer values.) We would consider the discrepancies $(x^*,y^*) - \proj_K((x^*,y^*))$ and their dispersions. 
%Under this paradigm, bounding is applied to $K$.
\item Viewing the node subproblems as instances of problem~\eqref{D1}, assuming we have a dual optimal solution $\omega^*$, we have corresponding sets of primal certificate solutions $K^*(\omega^*)$. For these solutions, we have $K^*(\omega^*) \cap  Z = \emptyset$ in the case of duality gap $\phi_C^* < f^*$. We consider one of two projections:
\begin{enumerate}
\item Take the projection $\proj_Z(K^*(\omega^*))$. Conceptually, we want to accumulate the dispersions in some set-wise or point-wise fashion.
Ideally, we do not consider the dispersion of only one $(\widehat{x},\widehat{y}) \in K^*(\omega^*)$, but the entire set $K^*(\omega^*)$, which is contained in the highest dimensional face of $\conv(K)$ for which $(x^*,y^*)$ is in its relative interior.
\item Rather than projection, we use the information from $x^*$, $y^*$, $z^*$ and $\omega^*$. (Or, more practically, $x^k$, $y^k$, $z^k$ and $\omega^k$.) We still work with the set $K^*(\omega^k)$, whose vertex description is available from computing $(\widehat{x}^t,\widehat{y}^t) \in K^*(\omega^t)$ at each iteration $t=1,\dots,k$.
%Take the projection of $Q\widehat{x}(\omega^*)$ onto $Q\conv(X) \cap Z$, denoted by 
%$$P_{Q\conv(X) \cap Z}(Q\widehat{x}(\omega^*)).$$ We shall show that
%$z^*$ is an element of $P_{Q\conv(X) \cap Z}(Q\widehat{x}(\omega^*))$, and so we consider the discrepancies $Q\widehat{x}(\omega^*)-z^*$.
\end{enumerate}
%Under this paradigm bounds are applied (conceptually) to $Z$, but in implementational practice, still on $X$.
\end{enumerate}



Due to the large number of variables that would be branched on under the first approach, branching is not based on the first approach. Although we do use the dispersions from the first approach discrepancies to inform branching based on the second approach. 
%Thus, conceptually, we branch in the ambient space of $Z$.
Branching occurs on components $x_{i,j}$ of each $x_i$ that either violate integrality constraints, or that show dispersion from some consensus value associated with other violated constraints.
The constraints added due to branching are denoted $x \in B$. Based on this constraint, define $K_B:=\braces{(x,y) \in K : x \in B}$, so that we define, respectively,
\begin{equation}\label{P1B}
f_B^*:=\min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ c^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in K_i,\; x_i \in B_i,\;  x_i=z \text{ for }i=1,\dots,m,\; z \in \mathbb{R}^{n_1}},
\end{equation}
\begin{equation}\label{D1B}
\phi_{B}^*:=\max_{\omega \in \Omega} \phi_B(\omega).
\end{equation}
with
\begin{equation}\label{PhiCB}
\phi_B(\omega) = \min_{x,y} \braces{ \sum_{i=1}^m p_i \left[ \left(c+\omega_i\right)^\top x_i + d_i^\top y_i \right] : (x_i,y_i) \in K_i,\; x_i \in B_i \; \forall i=1,\dots,m}
\end{equation} 
and
\begin{equation}\label{D2B}
\phi_{B}^*=\min_{x,y,z} \braces{\sum_{i=1}^m p_i \left[ c^\top x_i + d_i^\top y_i \right] : \begin{array}{c}(x_i,y_i) \in \conv(K_i \cap B_i),\\  x_i=z \text{ for }i=1,\dots,m,\\ z \in \mathbb{R}^{n_1}\end{array}}.
\end{equation}
%Note that we may use either the constraints $z \in B$ or $Qx \in B$ interchangeably as the context warrants.




During each node processing of the branch-and-bound, an attempt is made to find a feasible solution to the node-specific instance of~\eqref{P1B}, which provides a finite upper bound, which we shall refer to as the \emph{incumbent value} and denote as $\upsilon \in \mathbb{R}$. Furthermore, we use an iterative approach, referred to as the \emph{oracle}, to solving each instance of~\eqref{D1B} and~\eqref{D2B} simultaneously, that is guaranteed to converge optimally. During the processing of each node, one of three things will happen:
\begin{enumerate}
\item The oracle terminates due to the objective value exceeding the incumbent value (fathoming due to bound), or due to subproblem infeasibility.
%We shall see that this form of termination also catches all forms of subproblem infeasibility (as the infimum of~\eqref{EQBBSP} is infinity; this is where the need for a finite incumbent value arises).
\item The oracle terminates due to optimality of~\eqref{P1B} within pre-specified precision. 
That is $\norm{(x^*,y^*) - \proj_K(x^*,y^*)} + \sum_{i=1}^m \norm{x_i^* - z^*} < \epsilon$. The node is fathomed after testing whether $f_B^* < \upsilon$
and updating $\upsilon \gets f_B^*$ if that is the case. (In Caroe and Schultz, it is the vertex solutions $(\widehat{x},\widehat{y})$ that are tested only, so that the criterion is $\norm{\widehat{x}_i - \widehat{z}} < \epsilon$, where $\widehat{z}$ is some averaging of $\widehat{x}_i$, $i=1,\dots,m$.)
\item Otherwise, branchings need to be determined.
\end{enumerate}

\begin{algorithm}[H]
\begin{algorithmic}
 \State {\bf Definitions:} 
 \State \quad\quad$\mathcal{S}:=\braces{f,K,B}$, $\mathcal{P}:=\braces{\rho,\gamma, \epsilon, t_{max}, k_{max}}$
 \State \quad\quad$\upsilon$ is an incumbent value, and 
% \State \quad\quad$\left( \widetilde{x}, \widetilde{y}, \widetilde{z} \right)$ is a corresponding incumbent feasible solution for~\eqref{P1} for which $\upsilon = F(\widetilde{z})$
 \State {\bf Precondition:} 
 \State \quad\quad It is assumed that the value of the current node has been checked to not exceed the incumbent value $\upsilon$ 
\end{algorithmic}
\begin{algorithmic}
\Function{Process}{$\mathcal{S}$, $\mathcal{P}$, $\omega^0$, $\upsilon$, $\left( \widetilde{x}, \widetilde{y}, \widetilde{z} \right)$}
  \State $\left(K^{*}(\omega^*),(x^{*},y^*,z^{*}),\omega^{*},{\phi}^{*}\right) \gets \textbf{bound}(\mathcal{S}$, $\mathcal{P}$ $\omega^0)$
% \State $(D^0, x^0, z^0, \omega^0, \widecheck{\phi}^{0}) \gets$ Initialize($\mathcal{S},\left( \widetilde{x}, \widetilde{z} \right)$), $\textbf{term}\gets\textbf{false}$, $k \gets 0$
%\While{$\neg (\widecheck{\phi}^{k} > \upsilon \; \textbf{or}\; \textbf{term})$} 
% \State $(\widehat{x}^{k+1},x^{k+1},z^{k+1},\omega^{k+1},\widecheck{\phi}^{k+1}, \textbf{term}) \gets \text{PSCG}(\mathcal{S},\mathcal{P},D^k,x^k,z^k,\omega^k,\widecheck{\phi}^k)$
% \State $k \gets k+1$
%\EndWhile
\If{${\phi}^{*} > \upsilon$ \textbf{or} ${\phi}^* = \infty$}
 \State \textbf{fathom node}
 \State $\mathcal{B} \gets \emptyset$
 \State \textbf{return} $(\upsilon, \widetilde{z}, \mathcal{B})$
\EndIf
\State $\mathcal{B} \gets \textbf{findBranchings}(K^{*}(\omega^*),(x^{*},y^*,z^{*}),\omega^{*})$
\State $(\widetilde{x},\widetilde{y},\widetilde{z},\upsilon) \gets \textbf{findFeasibleSolution}(z^{*},\omega^{*},\mathcal{S},\mathcal{P})$ %(Note that any solution $\widetilde{z}$ found will satisfy $\widetilde{z} \in B$.)
\If{$\mathcal{B} \neq \emptyset $}
\State \textbf{node needs to branch}
\Else
\State \textbf{fathom node}  (Due to optimality)
\EndIf
\State \textbf{return} $(\widetilde{x},\widetilde{y},\widetilde{z},\upsilon), \mathcal{B})$
\EndFunction
\Function{bound}{$\mathcal{S}$, $\mathcal{P}$ $\omega^0$}
\State $(D^0,x^0,z^0,\omega^0,{\phi}^0) \gets$ Initialize($\mathcal{S},\mathcal{P},\omega^0$)
\State $k \gets 0$, $\textbf{term}\gets\textbf{false}$
\While{$\neg (\widecheck{\phi}^{k} > \upsilon \; \textbf{or}\; \textbf{term})$} 
 \State $(K^*(\omega^{k+1}), (x^{k+1},y^{k+1},z^{k+1}),\omega^{k+1},{\phi}^{k+1}, \textbf{term}) \gets \text{PSCG}(\mathcal{S},\mathcal{P},D^k,x^k,z^k,\omega^k,\widecheck{\phi}^k)$
 \State $k \gets k+1$
\EndWhile
\State \textbf{return} $\left(K^*(\omega^{k}), (x^{k},y^{k},z^{k}),(\widetilde{x},\widetilde{y},\widetilde{z}),\omega^{k},{\phi}^{k},\upsilon\right)$
\EndFunction
\end{algorithmic}
\end{algorithm}



\subsection{Parallel Stabilized Column Generation (PSCG)}

In applying the AL method to problem~\eqref{D2}, 
the continuous master problem for fixed $\omega \in Z^\perp$ takes the form
\begin{equation}\label{EqQMPConv}
\phi_\rho^{AL}(\omega):=\min_{x,y,z} \braces{L_\rho(x,y,z,\omega), (x,y) \in \conv(K), z \in Z}
\end{equation}
where the augmented Lagrangian (AL) relaxes $x_i=z$, $i=1,\dots,m$ and is defined by
\begin{equation}\label{ALDefn}
L_\rho(x,y,z,\omega):= \sum_{i=1,\dots,m} p_i\brackets{(c+\omega_i)^\top x_i + d_i^\top y_i + \frac{\rho}{2}\norm{x_i-z}_2^2}.
\end{equation}







In the algorithm that follows, we use the following approximation $\widehat{\phi} : \mathbb{R}^q \times \mathbb{R}^n \times \mathbb{R}^q \mapsto \mathbb{R}$ of $\phi^C$ centered at $(x^k,z^k)$, $k \ge 0$, in place the cutting plane model:
\begin{align*}
\widehat{\phi}(\omega,x^k,y^k, z^k) := L_\rho(x^k,y^k,z^k,\omega) + \frac{\rho}{2} \sum_{i=1,\dots,m}\norm{x_i^k-z^k}_2^2.
\end{align*}
%where the augmented Lagrangian (AL) $L_\rho(x,z,\omega)$ is defined as in~\eqref{ALDefn}. 



%The convex hull $\conv(X)$ is not known explicitly, and so $\phi^C$ cannot be evaluated directly. 
%Consequently, we additionally make use of the following minorization $\widecheck{\phi}$ of $\phi^C$ that can be evaluated.
%For $x^k \in \conv(X)$, $k \ge 0$, define $\widecheck{\phi}(\omega,x^k)$ as follows:
%\begin{equation}\label{PhiCheck}
%\widecheck{\phi}(\omega,x^k) := \min_x \braces{f(x^k) + \nabla_x f(x^k)(x-x^k) + \omega^\top Qx : x \in X}.
%\end{equation}
%%Note that the $\conv$ is dropped due to the linear form of the objective function, and so the evaluation of $\widecheck{\phi}$ is practically implementable. 
%Observe that, due to the linearity of the objective function with respect to $x$ in~\eqref{PhiCheck}, the use of constraint sets $X$ and $\conv(X)$ are interchangeable, 
%and so in evaluating $\widecheck{\phi}$, an explicit description of  $\conv(X)$ is not required. 
%Furthermore, from the definition of $\phi^C$, the convexity of $f$ over $\mathbb{R}^n$, and the interchangeability of $X$ and $\conv(X)$ in~\eqref{PhiCheck}, it is clear that for all $x^k \in \mathbb{R}^n$, $k \ge 0$, we have $\phi^C(\omega) \ge \widecheck{\phi}(\omega,x^k)$. Furthermore, when $f$ is linear, we have $\phi^C(\omega) \equiv \widecheck{\phi}(\omega,x^k)$ for all $x^k$, $k \ge 0$; the two functions collapse into the same function with the centering at $x^k$ of the latter function now  irrelevant. 

In \cite{previouspaper}, we developed an efficiently parallelizable iterative procedure to solving problem~\eqref{P2}. 
In this paper, we incorporate this procedure as part of the bounding mechanism within a branch-and-bound approach....

For review purposes, the iterative dual procedure is given as follows.

\begin{algorithm}[H]
\caption{A regular iteration of PSCG.\label{AlgDualAscentGSSSC}}
\begin{algorithmic}
\State {\bf Preconditions:} 
\State \quad\quad$(x^k,y^k) \in \conv(K)$, $z^k \in \argmin_z \braces{\sum_{i=1,\dots,m}\norm{x_i-z}^2}$, $\omega^k \in \Omega$, 
\State \quad\quad${\phi}^k = {\phi}(\omega^k)$, $\braces{(x^k,y^k) + \alpha ((\widehat{x},\widehat{y}) - (x^k,y^k)) : \alpha \in [0,1]} \subseteq {D} \subseteq \conv(K)$ where
\State \quad\quad\quad\quad$(\widehat{x},\widehat{y}) \in \argmin_{x,y} \braces{\nabla_{x,y} L_\rho(x^k,y^k,z^k,\omega^k) [(x - {x}^k);(y-y^k)] : (x,y) \in K}$.
\State \quad\quad$\mathcal{S}:=\braces{c,d,K,B}$, $\mathcal{P}:=\braces{\rho,\gamma, \epsilon, t_{max}, k_{max}}$
\end{algorithmic}
\begin{algorithmic}[1]
\Function{PSCG}{$\mathcal{S}$, $\mathcal{P}$, $D^k$, $x^k$, $y^k$, $z^k$, $\omega^k$, ${\phi}^{k}$ }  
       \For{$k=1,2,\dots,k_{max}$}
       \State Initialize $\omega^{k+1} \gets \omega^{k}$, ${\phi}^{k+1} \gets {\phi}^{k}$ \Comment{(Default, null-step updates)}
       \State $(\widehat{x}^{k+1},(x^{k+1},y^{k+1},z^{k+1}), D^{k+1},\Gamma) \gets$ SDM-GS($L_\rho(\cdot,\cdot,\cdot,\omega^k)$, $K \cap B$, $D^k$, $x^{k}$, $y^k$, $z^{k}$, $t_{max}$) \label{ReturnSDMGS}
       \If{$L_\rho({x}^{k+1},y^{k+1},{z} ^{k+1},\omega^k) + \frac{\rho}{2} \sum_{i=1}^m\norm{{x}_i^{k+1}-{z}^{k+1}}_2^2-{\phi}^k \le \epsilon$} \label{Alg4BeginTermCond}
       	\State {\bf return} $(x^{k+1},y^{k+1},z^{k+1},\omega^{k+1},{\phi}^{k+1}, D^{k+1}, \textbf{true})$
       \EndIf \label{Alg4EndTermCond}
       \State $\widetilde{\phi} \gets  L_{\rho}({x}^{k+1},y^{k+1},{z} ^{k+1},\omega^k) + \frac{\rho}{2} \sum_{i=1}^m\norm{{x}_i^{k+1}-{z}^{k+1}}_2^2 - \Gamma$ \label{ComputePhiTilde}
       \State $\widetilde{\gamma} \gets \frac{\widetilde{\phi} - {\phi}^k}{L_\rho({x}^{k+1},y^{k+1},{z}^{k+1}, \omega^k) + \frac{\rho}{2} \sum_{i=1}^m\norm{{x}_i^{k+1}-{z} ^{k+1}}_2^2-{\phi}^k}$ \label{ComputeCritVal}
       \If{$\widetilde{\gamma} \ge \gamma$} \label{Alg4LineSSC}
         \State set $\omega^{k+1} \gets \omega^{k} + \rho ({x}^{k+1}-{z}^{k+1})$, ${\phi}^{k+1} \gets \widetilde{\phi}$
        \EndIf
        \State Possibly update $\rho$, e.g., $\rho \gets \frac{1}{\min \braces{\max \braces{(2/\rho)(1-\widetilde{\gamma}),1/(10\rho),10^{-4}}, 10/\rho}}$ as in~\cite{Kiwiel1995} \label{Alg4RhoUpdate}
        \EndFor  
      \State \textbf{return}  $(\widehat{K}(\omega^{k+1}),(x^{k+1},y^{k+1},z^{k+1}),\omega^{k+1},{\phi}^{k+1}, D^{k+1}, \textbf{false})$
\EndFunction
\end{algorithmic}
\medskip
\begin{algorithmic}
\State {\bf Preconditions:} $(\widetilde{x},\widetilde{y}) \in \conv(K)$, $\widetilde{z} \in \argmin_z \braces{F(\widetilde{x},\widetilde{y},z) : z \in Z}$, $D \subseteq \conv(K)$
\end{algorithmic}
\begin{algorithmic}[1]
\Function{SDM-GS}{$F$, $X$, $Z$, $D$, $\widetilde{x}$, $\widetilde{z}$, $t_{max}$}  
        \For{$t=1,\dots,t_{max}$} \label{ForXYUpdateBegin}
           \State $\widetilde{x} \gets \argmin_x \braces{F(x, y,\widetilde{z}) : (x,y) \in D}$ \label{SDMGSXUpdate}
	\State $\widetilde{z} \gets \argmin_z \braces{F(\widetilde{x},\widetilde{y},z) : z \in Z}$ \label{SDMGSZUpdate}
        \EndFor \label{ForXYUpdateEnd}
	\State $(\widehat{x},\widehat{y}) \in \argmin_{x,y} \braces{\nabla_{x,y} F(\widetilde{x},\widetilde{y},\widetilde{z}) [(x - \widetilde{x});(y-\widetilde{y})] : (x,y) \in K}$ \label{SCGDirFindingSP}
	 \State Reconstruct ${D}$ to be any set such that \label{LineDa}
	 \State \quad $\braces{(\widetilde{x},\widetilde{y}) + \alpha ((\widehat{x},\widehat{y}) - (\widetilde{x},\widetilde{y})) : \alpha \in [0,1]} \subseteq {D} \subseteq \conv(K)$ \label{LineDb}
	 \State Set $\Gamma \gets - \nabla_{x,y} F(\widetilde{x},\widetilde{y},\widetilde{z}) [(\widehat{x} - \widetilde{x});(\widehat{y} -\widetilde{y})]$ \label{GammaLine}
      \State \textbf{return} $\left(\widehat{K},\widetilde{x},\widetilde{y},\widetilde{z},{D}, \Gamma \right)$ 
\EndFunction
\end{algorithmic}
\end{algorithm} 



%Algorithm PSCG addresses the solution to an alternative dual problem which is equivalent to~\eqref{P2} when $f$ is linear, but in general provides a weaker dual bound otherwise. This dual problem is used to address the more general setting where $f$ is convex but possibly nonlinear.%, so that $f_X^{**}$ never arises in the analysis or algorithm statement and need not be computed.




\begin{proposition}\label{PropFWBCD}
Let $\braces{(x^k,y^k,z^k,\omega^k)}$ be a sequence generated by Algorithm~\ref{AlgDualAscentGSSSC} applied to problem~\eqref{P1} with $X$ compact, $Z$ a linear subspace, $\omega^0 \in \Omega$, $B$ closed and convex, $\rho > 0$, $\gamma \in (0,1)$, $\epsilon=0$ and $k_{max}=\infty$. 
If there exists a dual optimal solution $\omega^*$ to the dual problem~\eqref{D1}, then either
\begin{enumerate}
\item $\omega^k = \overline{\omega}$ is fixed and optimal for~\eqref{D1} for $k \ge \bar{k}$ for some finite $\bar{k}$; or
%\item finite termination where, for some $\bar{k} \ge 1$,  $\omega^*=\omega^{\bar{k}}$ and $(x^{\bar{k}},z^{\bar{k}})$ is optimal for problem~\eqref{P1ConvB}; or 
\item $\omega^k$ is never optimal for~\eqref{D1} for any finite $k \ge 1$, but $\lim_{k \to \infty} \omega^k = \overline{\omega}$ is optimal,
\end{enumerate}
and the sequence $\braces{(x^k,y^k,z^k)}$ has limit points $(\overline{x},\overline{y},\overline{z})$, each of which are optimal for problem~\eqref{D2}.
\end{proposition}

\subsection{Adjustment of penalties}

One important feature of PSCG is the dynamic adjustment of penalty for speeding up convergence. Adjustments using information from the test for the serious step condition are used.

%\subsection{Parallelization and workload}
%The opportunities for parallelization and distribution of the computational workload in PSCG, as stated in Algorithm~\ref{AlgDualAscentGSSSC}, are not immediately apparent.
%This subsection explicitly indicates which update problems may be solved in parallel, and the nature of the required communication between the parallel computational nodes.
%
%The bulk of computational work, parallelization, and parallel communication occurs within the SDM-GS method stated in Algorithm~\ref{AlgBCD-XZ}, where for the problems of interest, the following decomposable structures apply: $X=\prod_{i=1}^m X_i$, $D=\prod_{i=1}^m D_i$,  and $F(x,z)=\sum_{i=1}^m F(x_i,z)$. 
%In the larger context of Algorithm~\ref{AlgDualAscentGSSSC}, the subproblem of Line~\ref{SDMGSXUpdate} in Algorithm~\ref{AlgBCD-XZ} can be solved in parallel given fixed $\widetilde{z} \in Z$ and $\omega \in Z^\perp$ along the block indices $i=1,\dots,m$ as
%\begin{equation}\label{Alg4QMP}
%\min_{x} \braces{ f_i(x) + (\omega_i)^\top Q_i x + \frac{\rho}{2} \norm{Q_i x - \widetilde{z}_i}_2^2 : x \in D_i},
%\end{equation}
%while the subproblem of Line~\ref{SCGDirFindingSP} is solved as
%$$
%\min_x \braces{ \nabla_x f_i(\widetilde{x}_i) + \left( \omega_i + \rho(Q_i \widetilde{x}_i - \widetilde{z}_i) \right)^\top Q_i x : x \in X_i}.
%$$
%\begin{remark}
%In the setting where problem~\eqref{P1} is a large-scale mixed-integer linear optimization problem, the subproblems of Line~\ref{SDMGSXUpdate} are continuous convex quadratic optimization problems for each block $i=1,\dots,m$, which can be solved independently of one another and in parallel. In the same setting, the Line~\ref{SCGDirFindingSP} subproblems are mixed-integer optimization problems for each block $i=1,\dots,m$, which can also be solved independently of one another and in parallel.
%Additionally, the reconstruction of $D$ occurring in Line~\ref{LineDb} can be done in parallel for each $D_i$ along the indices $i=1,\dots,m$.
%\end{remark}


%Parallel communication is needed for the computation of the $z$ update. In the larger context of Algorithm~\ref{AlgDualAscentGSSSC}, this takes the form of solving
%$$
%\min_z \braces{\sum_{i=1}^m \norm{Q_i \widetilde{x}_i - z_i}_2^2 : z \in Z}.
%$$
%This is solved as an averaging that requires the reduce-sum type parallel communication.
%The computation of values required to compute $\gamma^k$ in Line~\ref{ComputeCritVal} in Algorithm~\ref{AlgDualAscentGSSSC} also requires a reduce-sum type parallel communication. 
%For implementation purposes, the computation of these values, including the computation of $\Gamma$ from the SDM-GS call, can be combined into one reduce-sum communication. In total, each iteration of Algorithm~\ref{AlgDualAscentGSSSC} requires two reduce-sum type communications, one for computing the $z$-update, and one combined reduce-sum communication to compute scalars associated with the Lagrangian bounds and the critical values for the termination conditions. 
%The storage and updates of $x^k$ and $\omega^k$ and $D$ can also be done in parallel, while $z^k$ and $\gamma^k$ need to be computed and stored by every processor at each iteration $k$.

%%%%%%%%%%%%%%%%%%%% BRANCH-AND-BOUND %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Branch-and-bound with SCG-GS-ALM as bounding procedure}

In order to inform branching decisions, we need a sense of dispersion based on some violation of feasibility, and a value with which to bound.

\subsection{Measure of feasibility}

In the baseline DD branch-and-bound, a criterion for fathoming by optimality may be given by
\begin{equation}\label{FeasTolDDBasic}
\sqrt{\sum_{i=1}^m \norm{\widehat{x}_i-\widehat{z}}_2^2} < \epsilon,
\end{equation}
for some suitably small tolerance $\epsilon > 0$,
where only the deviations from satisfaction of $\widehat{x}_i=\widehat{z}$, $i=1,\dots,m$, $\widehat{z} \in \proj_Z(\widehat{x})$, are taken into account since the integrality constraints on $\widehat{x}$ are satisfied by construction. We define for convenience the vector in $\mathbb{R}^n$:
$$
\sigma_Z:=\brackets{\sum_{i=1}^m \norm{\widehat{x}_{i,j}-\widehat{z}_j}}_{j=1,\dots,n}.
$$

On the other hand, testing feasibility based on $(x^*,y^*)$, we take
\begin{equation}\label{FeasTolDDImproved}
\sqrt{\sum_{i=1}^m \norm{(x^*,y^*)-\proj_K((x^*,y^*))}_2^2} < \epsilon,
\end{equation}
for some suitably small tolerance $\epsilon > 0$,
where it is assumed that the satisfaction of $x^*_i = z^*$ for each $i=1,\dots,m$ can be realized through the underlying dual iterative solution process at each node. We define for convenience the vector in $\mathbb{R}^m$:
$$
\sigma_K:=\left[\norm{(x_i^*,y_i^*)-\proj_{K_i}((x_i^*,y_i^*))} \right]_{i=1,\dots,m}.
$$

 If either of the feasibility criteria~\eqref{FeasTolDDBasic} or~\eqref{FeasTolDDImproved} are not met, then branching must occur, and the branching should be informed by the dispersions measured in the respective norm terms of~\eqref{FeasTolDDBasic} and~\eqref{FeasTolDDImproved}. 


\subsection{Branching decisions}
We have the following approaches at measuring dispersion from the constraint $x_i=z$, $i=1,\dots,m$, at iteration $k$:
\begin{enumerate}
\item Given $\widehat{x}^k \in K^*(\omega^k)$ for some dual feasible solution $\omega^k$, and 
setting $\widehat{x}^k=\brackets{\widehat{x}_i^k}_{i=1,\dots,m}$, we take an average $\widehat{z}^k = \proj_Z(\widehat{x}^k)$. Based on this average,
we compute a component-wise dispersion 
$$\sigma_j^k:=\sum_{i=1,\dots,m} \left| \brackets{\widehat{x}_i^k - \widehat{z}^k}_j \right|$$% +  \left| \brackets{Q_i{x}_i^k - {z}_i^k}_j \right|$$ 
for each component index $j=1,\dots,n_x$, and denote $\sigma:=\brackets{\sigma_j}_{j=1,\dots,n_x}$. This is the approach [in Caroe and Schultz]. One quirk with this approach is that, at the optimal dual solution $\omega^*$, any of the previously generated solutions $\widehat{x} \in K^*(\omega^k)$ may also satisfy $\widehat{x} \in K^*(\omega^*)$. 
%This is especially true in the typically satisfied case that $c \in Z$. 
%Also, the approach does not use information from the dispersions $\norm{(x^*,y^*) - \proj_K((x^*,y^*))}$. The latter issue is addressed with the following modification to $\sigma_j^k$:
%$$
%\sigma_j^k:=\norm{ \left[([\sigma_X]_i+\epsilon)[Q_i\widehat{x}_{i}^k) - \widehat{z}_i^k]_j\right]_{i=1,\dots,m} } + \norm{Qx^k-z^k}
%$$
\item Let $\widehat{x}^t \in K^*(\omega^t)$ for $t=0,1,\dots,k$. We have for each $i=1,\dots,m$ that $x_i^k = \sum_{t} \alpha^t \widehat{x}^t$. Furthermore, we take 
$$\widecheck{z} = \sum_{i=1}^m \beta_i x^i.$$ 
where we assume that the weights $\beta \ge 0$, $i=1,\dots,m$, have been normalized so that $\sum_{i=1}^m \beta_i = 1$.
Then we define the dispersions
$$
\sigma_j := \sum_{i=1,\dots,m} \beta_i \sum_{t=1,\dots,T_i} \alpha_t \left|\widehat{x}_{i,j}^t - \widecheck{z}_{j}^k \right| 
$$
%$$ \sigma_j := \frac{1}{\sum_{i=1} [\sigma_X]_i+\epsilon}\sum_{i=1,\dots,m} [\sigma_X+\epsilon]_i \sum_{t=1,\dots,k} \alpha_t \left|Q_i\left( \widehat{x}_{i,j}^t - x_{i,j}^k \right) \right| + \rho_{i,j}\left| Q_i x_{i,j}^k - z_j^k \right|, 
%$$
where $0 \le \alpha_t \le 1$ for $t=1,\dots,T_i$ and $\sum_{t=1,\dots,T_i} \alpha_t = 1$ are the convex multipliers used to form a convex combination $x^k = \sum_{t=1,\dots,k} \alpha_t\widehat{x}^t$. The weights $\beta_i \ge 0$ are chosen by some "sensible" rationale, for example, using information from the feasibility measure, we might choose $\beta_i = \sigma_{K,i}$ for $i=1,\dots,m$. In the case of branching on continuous variables, we add the branch constraints $x_j \le \widecheck{z}_j$ and $x_j \ge \widecheck{z}_j$; while for integer-restricted variables, we add $x_j \le \lfloor \widecheck{z}_j \rfloor$ and $x_j \ge \lceil \widecheck{z}_j \rceil$. Notice that $\widecheck{z}$ and $z^*$ are not the same.
\end{enumerate}

\begin{remark}
We also considered using a measure of the dispersion of the dual solutions $\omega^k$, taking
$$
\sigma_j := \norm{\left[\omega_{i,j}\right]_{i=1,\dots,m}}.
$$
This approach is (somewhat) sensible for when restricting the branching to variables with integrality restrictions, or when the disjunctions are otherwise strictly separated. It is otherwise an ill-defined approach, in that optimal dual solutions are typically non-unique, especially when bounding constraints are being added. Any further thoughts on this matter must take into account the set of dual optimal solutions.
\end{remark}

\section{Computational results}

%\begin{tabular}{|c|rr|rr|}
%\hline
%& \multicolumn{4}{|c|}{DCAP 233-500}\\
% \hline(
%Max. & \multicolumn{1}{c}{Best node value} & \multicolumn{1}{c}{Incumbent value} & \multicolumn{1}{c}{SDM-GS1-ALM} & \multicolumn{1}{c|}{SDM-GS5-ALM} \\
%walltime (mins)  & \multicolumn{1}{c}{OOQP} & \multicolumn{1}{c}{PIPS-IPM} & \multicolumn{1}{c}{SDM-GS1-ALM} & \multicolumn{1}{c|}{SDM-GS5-ALM} \\
%\hline
%5		& (68, 16.15)		& (66, 12.71) 	& (68, 3.67) 		& (68, 5.21)	\\
%10		& (68, 6.62)			& (70, 2.39) 		& (68, 0.53) 		& (68, 0.64)	\\
%20		& (68, 5.75)			& (73, 1.56) 		& (68, 0.28) 		& (68, 0.33)	\\
%40		& (68, 9.91)			& (70, 1.24) 		& (68, 0.16) 		& (68, 0.19)	\\
%\hline
%\end{tabular}
\begin{footnotesize}
\begin{tabular}{cccc|cccc}
\multicolumn{8}{c}{DCAP233500 (using 32 processors)}\\
\multicolumn{4}{c}{Baseline DD} & \multicolumn{4}{c}{Improved DD}\\
Time & $\phi_{best}$ & $f_{best}$ & Nodes & Time & $\phi_{best}$ & $f_{best}$ & Nodes \\
\hline
5 min & 1737.48989 & 1740.64181 & 56 & 5 min & 1737.51795 & 1737.52526 & 35 \\
10 min & 1737.49820 & 1740.64181 & 79 & 10 min & 1737.52047 & 1737.52069 & 64 \\
20 min & 1737.50103 & 1740.64181 & 102 & 20 min* & 1737.52069 & 1737.52069 & 157 \\
40 min & 1737.50154 & 1737.53164 & 141 & 40 min* & ********* & ********* & *** 
\end{tabular}

\medskip
\begin{tabular}{cccc|cccc}
\multicolumn{8}{c}{DCAP243500 (using 32 processors)}\\
\multicolumn{4}{c}{Baseline DD} & \multicolumn{4}{c}{Improved DD}\\
Time & $\phi_{best}$ & $f_{best}$ & Nodes & Time & $\phi_{best}$ & $f_{best}$ & Nodes \\
\hline
5 min &   2167.05580 & 2176.36500 & 44 & 5 min & 2167.31697 & 2169.47892 & 31 \\
10 min & 2167.14788 & 2171.72801 & 57 & 10 min & 2167.33071 & 2167.36434 & 42 \\
20 min & 2167.15808 & 2171.30679 & 74 & 20 min & 2167.34274 & 2167.35855 & 65\\
40 min & 2167.19891 & 2169.36526 & 89 & 40 min & 2167.34750 & 2167.35373 & 104 
\end{tabular}

\medskip
\begin{tabular}{cccc|cccc}
\multicolumn{8}{c}{DCAP332500 (using 32 processors)}\\
\multicolumn{4}{c}{Baseline DD} & \multicolumn{4}{c}{Improved DD}\\
Time & $\phi_{best}$ & $f_{best}$ & Nodes & Time & $\phi_{best}$ & $f_{best}$ & Nodes \\
\hline
5 min &   1587.39480 & 1681.65011 & 50 & 5 min & 1588.08901 & 1616.57914 & 32 \\
10 min & 1587.79126 & 1638.79564 & 71 & 10 min & 1588.40129 & 1591.17931 & 55 \\
20 min & 1587.93833 & 1594.89727 & 97 & 20 min & 1588.60162 & 1590.96571 & 91\\
40 min & 1588.18939 & 1593.11529 & 158 & 40 min & 1588.67017 & 1590.77970 & 128 
\end{tabular}

\medskip
\begin{tabular}{cccc|cccc}
\multicolumn{8}{c}{DCAP342500 (using 32 processors)}\\
\multicolumn{4}{c}{Baseline DD} & \multicolumn{4}{c}{Improved DD}\\
Time & $\phi_{best}$ & $f_{best}$ & Nodes & Time & $\phi_{best}$ & $f_{best}$ & Nodes \\
\hline
5 min &   1904.33558 & 1922.89408 & 55 & 5 min & 1904.30712 & 1912.75878 & 43 \\
10 min & 1904.43416 & 1910.50410 & 104 & 10 min & 1904.49445 & 1908.06074 & 64 \\
20 min & 1904.44938 & 1910.50410 & 140 & 20 min & 1904.63236 & 1905.78144 & 112\\
40 min & 1904.46666 & 1904.70152 & 206 & 40 min & 1904.65008 & 1904.67035 & 166
\end{tabular}

%\medskip
%\begin{tabular}{cccc|cccc}
%\multicolumn{8}{c}{DCAP332500 (using 32 processors)}\\
%\multicolumn{4}{c}{Baseline DD} & \multicolumn{4}{c}{Improved DD}\\
%Time & $\phi_{best}$ & $f_{best}$ & Nodes & Time & $\phi_{best}$ & $f_{best}$ & Nodes \\
%\hline
%5 min &   * & * & 44 & 5 min & * & * & * \\
%10 min & * & * & * & 10 min & * & * & * \\
%20 min & * & * & * & 20 min & * & * & *\\
%40 min & * & * & * & 40 min & * & * & * 
%\end{tabular}

\end{footnotesize}



%%%%%%%%%%%%%%%%%%%% CONCLUSION AND FUTURE WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and future work}\label{SectConclusion}
%Our contribution is motivated by the goal of improving the efficiency of parallelization applied to iterative approaches for solving the Lagrangian dual problem of 
%large scale optimization problems. These problems have nonlinear convex differentiable objective $f$, decomposable nonconvex constraint set $X$, and nondecomposable affine constraint set $Qx=z$ to which Lagrangian relaxation is applied. Problems of such a form include the split variable extensive form of mixed-integer linear stochastic programs as a special case. 
%%mixed-integer linear stochastic optimization problems. 
%%In considering the split variable extensive deterministic form of such mixed-integer linear problems, 
%%The primal characterization of the Lagrangian dual problem of the form $\min_{x,z} \braces{f(x) : Qx=z, x \in \conv(X), z \in Z}$. 
%Implicitly, our approach refers to the convex hull $\conv(X)$ of $X$, and the assumed lack of known description of $\conv(X)$ needs to be addressed. Proximal bundle methods (alternatively in the form of the proximal simplicial decomposition method or stabilized column generation) are well-known for addressing the latter issue. In the former issue, that of exploiting the large scale structure to apply parallel computation efficiently, we develop a modified augmented Lagrangian (AL) method with approximate subproblem solutions that incorporates ideas from the proximal bundle method.% or more especially its dual form the proximal simplicial decomposition method.  
%
%%In developing this framework, we use the proximal simplicial decomposition method as a motivating analogy, where there is no use of a cutting-plane model. 
%%To this end, we develop an implementation of the augmented Lagrangian (AL) method with approximate subproblem solutions that is practical for our motivating problems and that allows for efficient parallelization. 
%The approximation of subproblem solutions is based on an iterative approach that integrates ideas from the simplicial decomposition method (SDM) (for constructing inner approximations of $\conv(X)$) and the nonlinear block Gauss-Seidel method. It is the latter Gauss-Seidel aspect that is primarily responsible for enhancing the parallel efficiency that is observed in the numerical experiments. While convergence analysis of the integrated SDM-GS approach may be derived from slight modifications to results in~\cite{Bonettini2011}, for the sake of completeness and explicitness, we provide in the appendix a proof of optimal convergence of SDM-GS as it is applied within our algorithm under a standard set of conditions. A distinction between so-called ``serious'' steps and ``null'' steps, in analogy to the proximal bundle method, is also recovered. Once these aspects are successfully integrated, then the contribution is complete, where the beneficial stabilization associated with proximal point methods and the ability to apply parallelization more efficiently are both realized. The resulting algorithm developed in this paper is referred to as SDM-GS-ALM, which has similar functionality to the alternating direction method of multipliers (ADMM).
%
%We performed numerical tests of two sorts. 
%%In Test 1, we examined the effect of approximating the solution of the AL primal subproblem using one iteration of SDM-GS against using exact solves of the proximal bundle method continuous master problem. In the setting with linear objective function, the latter computations were simulated by allowing the Gauss-Seidel iterations to solve the continuous master problem (with fixed inner approximation) to (near) exactness. We saw that in the early iterations, there was surprisingly no consistent outperforming of one method over the other in terms of the quality of Lagrangian bounds computed. In the tail-end iterations, the benefit of having the continuous master problem associated with the proximal bundle method solved exactly became more evident. %Therefore, enhancements to SDM-GS-ALM would have a rule introduced for adjusting the number of Gauss-Seidel iterations to use that are not followed by SDM expansions of the inner approximation.
%In Test 1, we examined the impact of varying the serious step condition parameter. We found that parameterizations that effect more stringent serious step conditions seem to have the effect of mitigating the early iteration instability due to penalty parameters that are too large. At the same time, the more stringent serious step condition parameterizations seemed to result in slower convergence to dual optimality in the tail-end. As is the case for proximal bundle methods, information obtained in the serious step condition tests may be used to beneficially adjust the proximal term penalty coefficient in   early iterations. 
%%(Although the convergence analysis may require the penalty to stabilize in the later iterations.)
%
%In Test 2, we examined the efficiency of parallelization, measured by the speedup ratio, due to the use of the SDM-GS-ALM, compared versus pre-existing implementations of the proximal bundle method that use structure exploiting primal dual interior point methods to improve parallel efficiency. We saw in these results a promising increase in parallel efficiency due to the use of SDM-GS-ALM, where the increase in parallel efficiency is attributed primarily to the successful incorporation of Gauss-Seidel iterations. The results of the last problem tested, SSLP 10-50-2000, additionally suggested a benefit due to the ability of SDM-GS-ALM to distribute not just the workload, but also the use of memory. The vector of auxiliary variables $z$ is the only substantial block of data that needs to be stored and modified by all processors. In the context of stochastic optimization problems, this represents a modest communication bottleneck in proportion to the number of first-stage variables for two-stage problems, while for multistage problems, the amount of such data that must be stored by every processor and modified by parallel communication can increase exponentially with the number of stages.
%
%Potential future improvements include the following. While a default implementation of SDM-GS-ALM would have one Gauss-Seidel iteration per SDM-GS call, the Lagrangian bounds reported from the Test 2 experiments suggest that an improved implementation would have early iterations use one Gauss-Seidel iteration per SDM-GS call, but steadily increase the number of Gauss-Seidel iterations per SDM-GS call for the later iterations. This results in better Lagrangian bounds at termination. While these extra Gauss-Seidel iterations require extra parallel communication, the additional wall clock time required becomes increasingly marginal for larger problems where the cost of solving the SDM linearized subproblems associated with expanding the inner approximation increasingly outweighs the cost associated with computing the approximate solution of the continuous master problem and any required parallel communications. 
%
%A potentially large improvement to the speed of convergence, in terms of wall clock time, would be to incorporate into the analysis the degree to which the SDM linearized subproblem can be solved suboptimally and yet retain the optimal convergence. We expect that solving these subproblems exactly, particularly in the early iterations, is highly wasteful, and providing a theoretical basis for controlling the tolerance of solution inaccuracy would be of great value. Another potential avenue for future work is to extend the experimental analysis to multistage mixed-integer stochastic optimization problems and/or nonlinear problems, as the form of the problem addressed by SDM-GS-ALM is general enough to model these types of problems. 



%\begin{acknowledgements}
%We thank Prof. Jeffrey Linderoth for helpful input in the development and preparation of this manuscript.
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}






% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{./bibliographyPSCGDD}   % name your BibTeX data base


\end{document}
% end of file template.tex

